{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bef9228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99466a1",
   "metadata": {},
   "source": [
    "## 5years data with many feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8e14424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Change', '5MA', '5Disparity',\n",
       "       '20MA', '20Disparity', '60MA', '60Disparity', '120MA', '120Disparity',\n",
       "       'bol_high', 'bol_low', 'PPO', 'PVO', 'SO', 'Williams_R', 'EOM', 'MFI',\n",
       "       'NVI', 'OBV', 'ATR', 'UI', 'ADX', 'MACD', 'TRIX', 'CCI', 'RMI', 'VHF',\n",
       "       'Institutional_trading', 'Individual_trading', 'Foreign_trading',\n",
       "       'Short_volume', 'Short_left', 'Cos_sim', 'wil', 'buy', 'Fluctuation',\n",
       "       'label', 'C_O', 'H_L', 'H_C', 'C_L'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>Short_left</th>\n",
       "      <th>Cos_sim</th>\n",
       "      <th>wil</th>\n",
       "      <th>buy</th>\n",
       "      <th>Fluctuation</th>\n",
       "      <th>label</th>\n",
       "      <th>C_O</th>\n",
       "      <th>H_L</th>\n",
       "      <th>H_C</th>\n",
       "      <th>C_L</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>70300</td>\n",
       "      <td>71400</td>\n",
       "      <td>69900</td>\n",
       "      <td>70100</td>\n",
       "      <td>4931703</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>72880.0</td>\n",
       "      <td>0.961855</td>\n",
       "      <td>74760.0</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>...</td>\n",
       "      <td>1856147</td>\n",
       "      <td>0</td>\n",
       "      <td>69050.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>68300</td>\n",
       "      <td>70000</td>\n",
       "      <td>68300</td>\n",
       "      <td>69700</td>\n",
       "      <td>3465521</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>71740.0</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>...</td>\n",
       "      <td>1659421</td>\n",
       "      <td>0</td>\n",
       "      <td>69050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1700</td>\n",
       "      <td>300</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>68500</td>\n",
       "      <td>69600</td>\n",
       "      <td>67400</td>\n",
       "      <td>68100</td>\n",
       "      <td>4958303</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>70320.0</td>\n",
       "      <td>0.968430</td>\n",
       "      <td>74160.0</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>...</td>\n",
       "      <td>1337487</td>\n",
       "      <td>1</td>\n",
       "      <td>69350.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>2200</td>\n",
       "      <td>1500</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>67200</td>\n",
       "      <td>68900</td>\n",
       "      <td>67100</td>\n",
       "      <td>68100</td>\n",
       "      <td>3402570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69200.0</td>\n",
       "      <td>0.984104</td>\n",
       "      <td>73770.0</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>...</td>\n",
       "      <td>1160003</td>\n",
       "      <td>0</td>\n",
       "      <td>68300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>67900</td>\n",
       "      <td>68200</td>\n",
       "      <td>66200</td>\n",
       "      <td>66700</td>\n",
       "      <td>3571792</td>\n",
       "      <td>-0.020558</td>\n",
       "      <td>68540.0</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>73270.0</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>...</td>\n",
       "      <td>1188425</td>\n",
       "      <td>0</td>\n",
       "      <td>68800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1200</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close   Volume    Change      5MA   \n",
       "DATE                                                                 \n",
       "2019-03-04  70300  71400  69900  70100  4931703  0.001429  72880.0  \\\n",
       "2019-03-05  68300  70000  68300  69700  3465521 -0.005706  71740.0   \n",
       "2019-03-06  68500  69600  67400  68100  4958303 -0.022956  70320.0   \n",
       "2019-03-07  67200  68900  67100  68100  3402570  0.000000  69200.0   \n",
       "2019-03-08  67900  68200  66200  66700  3571792 -0.020558  68540.0   \n",
       "\n",
       "            5Disparity     20MA  20Disparity  ...  Short_left  Cos_sim   \n",
       "DATE                                          ...                        \n",
       "2019-03-04    0.961855  74760.0     0.937667  ...     1856147        0  \\\n",
       "2019-03-05    0.971564  74450.0     0.936199  ...     1659421        0   \n",
       "2019-03-06    0.968430  74160.0     0.918285  ...     1337487        1   \n",
       "2019-03-07    0.984104  73770.0     0.923139  ...     1160003        0   \n",
       "2019-03-08    0.973154  73270.0     0.910332  ...     1188425        0   \n",
       "\n",
       "                wil  buy  Fluctuation  label   C_O   H_L   H_C   C_L  \n",
       "DATE                                                                  \n",
       "2019-03-04  69050.0    0            0    0.0  -200  1500  1300   200  \n",
       "2019-03-05  69050.0    1            0    0.0  1400  1700   300  1400  \n",
       "2019-03-06  69350.0    1            0    0.0  -400  2200  1500   700  \n",
       "2019-03-07  68300.0    1            0    0.0   900  1800   800  1000  \n",
       "2019-03-08  68800.0    0            0    0.0 -1200  2000  1500   500  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('news_cos_stock_sk.csv')\n",
    "df.columns.values[0]='DATE'\n",
    "data=df.set_index('DATE')\n",
    "data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "758216a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fluctuation\n",
       "0    654\n",
       "1    581\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Fluctuation\"] = 0\n",
    "data.loc[data[\"Change\"] > 0, \"Fluctuation\"] = 1\n",
    "data[\"Fluctuation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1c16055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                S&P 500       NASDAQ         Gold        Oil      USD/KRW\n",
      "Date                                                                     \n",
      "2019-03-04  2792.810059  7577.569824  1284.800049  56.590000  1126.810059\n",
      "2019-03-05  2789.649902  7576.359863  1282.000000  56.560001  1128.150024\n",
      "2019-03-06  2771.449951  7505.919922  1284.900024  56.220001  1126.199951\n",
      "2019-03-07  2748.929932  7421.459961  1283.800049  56.660000  1127.000000\n",
      "2019-03-08  2743.070068  7408.140137  1297.000000  56.070000  1133.069946\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "\n",
    "# 데이터 가져오기\n",
    "sp500 = fdr.DataReader('^GSPC', '2019-03-04', '2024-02-29')\n",
    "nasdaq = fdr.DataReader('^IXIC', '2019-03-04', '2024-02-29')\n",
    "gold = fdr.DataReader('GC=F', '2019-03-04', '2024-02-29')\n",
    "oil = fdr.DataReader('CL=F', '2019-03-04', '2024-02-29')\n",
    "usdkrw = fdr.DataReader('USD/KRW', '2019-03-04', '2024-02-29')\n",
    "\n",
    "# 데이터프레임 병합\n",
    "data_eco = pd.concat([sp500['Close'], nasdaq['Close'], gold['Close'], oil['Close'], usdkrw['Close']], axis=1)\n",
    "data_eco.columns = ['S&P 500', 'NASDAQ', 'Gold', 'Oil', 'USD/KRW']\n",
    "\n",
    "# 결과 출력\n",
    "print(data_eco.head())\n",
    "\n",
    "isnadata=data_eco[data_eco.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6898fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>C_O</th>\n",
       "      <th>H_L</th>\n",
       "      <th>H_C</th>\n",
       "      <th>C_L</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>70300</td>\n",
       "      <td>71400</td>\n",
       "      <td>69900</td>\n",
       "      <td>70100</td>\n",
       "      <td>4931703</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>72880.0</td>\n",
       "      <td>0.961855</td>\n",
       "      <td>74760.0</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>200</td>\n",
       "      <td>2792.810059</td>\n",
       "      <td>7577.569824</td>\n",
       "      <td>1284.800049</td>\n",
       "      <td>56.590000</td>\n",
       "      <td>1126.810059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>68300</td>\n",
       "      <td>70000</td>\n",
       "      <td>68300</td>\n",
       "      <td>69700</td>\n",
       "      <td>3465521</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>71740.0</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1700</td>\n",
       "      <td>300</td>\n",
       "      <td>1400</td>\n",
       "      <td>2789.649902</td>\n",
       "      <td>7576.359863</td>\n",
       "      <td>1282.000000</td>\n",
       "      <td>56.560001</td>\n",
       "      <td>1128.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>68500</td>\n",
       "      <td>69600</td>\n",
       "      <td>67400</td>\n",
       "      <td>68100</td>\n",
       "      <td>4958303</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>70320.0</td>\n",
       "      <td>0.968430</td>\n",
       "      <td>74160.0</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>2200</td>\n",
       "      <td>1500</td>\n",
       "      <td>700</td>\n",
       "      <td>2771.449951</td>\n",
       "      <td>7505.919922</td>\n",
       "      <td>1284.900024</td>\n",
       "      <td>56.220001</td>\n",
       "      <td>1126.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>67200</td>\n",
       "      <td>68900</td>\n",
       "      <td>67100</td>\n",
       "      <td>68100</td>\n",
       "      <td>3402570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69200.0</td>\n",
       "      <td>0.984104</td>\n",
       "      <td>73770.0</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>2748.929932</td>\n",
       "      <td>7421.459961</td>\n",
       "      <td>1283.800049</td>\n",
       "      <td>56.660000</td>\n",
       "      <td>1127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>67900</td>\n",
       "      <td>68200</td>\n",
       "      <td>66200</td>\n",
       "      <td>66700</td>\n",
       "      <td>3571792</td>\n",
       "      <td>-0.020558</td>\n",
       "      <td>68540.0</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>73270.0</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1200</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>2743.070068</td>\n",
       "      <td>7408.140137</td>\n",
       "      <td>1297.000000</td>\n",
       "      <td>56.070000</td>\n",
       "      <td>1133.069946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close   Volume    Change      5MA   \n",
       "2019-03-04  70300  71400  69900  70100  4931703  0.001429  72880.0  \\\n",
       "2019-03-05  68300  70000  68300  69700  3465521 -0.005706  71740.0   \n",
       "2019-03-06  68500  69600  67400  68100  4958303 -0.022956  70320.0   \n",
       "2019-03-07  67200  68900  67100  68100  3402570  0.000000  69200.0   \n",
       "2019-03-08  67900  68200  66200  66700  3571792 -0.020558  68540.0   \n",
       "\n",
       "            5Disparity     20MA  20Disparity  ...  label   C_O   H_L   H_C   \n",
       "2019-03-04    0.961855  74760.0     0.937667  ...    0.0  -200  1500  1300  \\\n",
       "2019-03-05    0.971564  74450.0     0.936199  ...    0.0  1400  1700   300   \n",
       "2019-03-06    0.968430  74160.0     0.918285  ...    0.0  -400  2200  1500   \n",
       "2019-03-07    0.984104  73770.0     0.923139  ...    0.0   900  1800   800   \n",
       "2019-03-08    0.973154  73270.0     0.910332  ...    0.0 -1200  2000  1500   \n",
       "\n",
       "             C_L      S&P 500       NASDAQ         Gold        Oil   \n",
       "2019-03-04   200  2792.810059  7577.569824  1284.800049  56.590000  \\\n",
       "2019-03-05  1400  2789.649902  7576.359863  1282.000000  56.560001   \n",
       "2019-03-06   700  2771.449951  7505.919922  1284.900024  56.220001   \n",
       "2019-03-07  1000  2748.929932  7421.459961  1283.800049  56.660000   \n",
       "2019-03-08   500  2743.070068  7408.140137  1297.000000  56.070000   \n",
       "\n",
       "                USD/KRW  \n",
       "2019-03-04  1126.810059  \n",
       "2019-03-05  1128.150024  \n",
       "2019-03-06  1126.199951  \n",
       "2019-03-07  1127.000000  \n",
       "2019-03-08  1133.069946  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index = pd.to_datetime(data.index)\n",
    "merged_data = pd.merge(data, data_eco, left_index=True, right_index=True)\n",
    "merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d0eb0408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open        0\n",
       "High        0\n",
       "Low         0\n",
       "Close       0\n",
       "Volume      0\n",
       "           ..\n",
       "S&P 500    39\n",
       "NASDAQ     39\n",
       "Gold       38\n",
       "Oil        38\n",
       "USD/KRW     1\n",
       "Length: 51, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Open       0\n",
       "High       0\n",
       "Low        0\n",
       "Close      0\n",
       "Volume     0\n",
       "          ..\n",
       "S&P 500    0\n",
       "NASDAQ     0\n",
       "Gold       0\n",
       "Oil        0\n",
       "USD/KRW    0\n",
       "Length: 51, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.isna().sum()\n",
    "# 결측치를 'bfill'로 처리하기\n",
    "data_df_filled = merged_data.fillna(method='bfill')\n",
    "data_df_filled.isnull().sum()\n",
    "data=data_df_filled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd920c",
   "metadata": {},
   "source": [
    "## 변수 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41de63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>C_O</th>\n",
       "      <th>H_L</th>\n",
       "      <th>H_C</th>\n",
       "      <th>C_L</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>70300</td>\n",
       "      <td>71400</td>\n",
       "      <td>69900</td>\n",
       "      <td>70100</td>\n",
       "      <td>4931703</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>72880.0</td>\n",
       "      <td>0.961855</td>\n",
       "      <td>74760.0</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>200</td>\n",
       "      <td>2792.810059</td>\n",
       "      <td>7577.569824</td>\n",
       "      <td>1284.800049</td>\n",
       "      <td>56.590000</td>\n",
       "      <td>1126.810059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>68300</td>\n",
       "      <td>70000</td>\n",
       "      <td>68300</td>\n",
       "      <td>69700</td>\n",
       "      <td>3465521</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>71740.0</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1700</td>\n",
       "      <td>300</td>\n",
       "      <td>1400</td>\n",
       "      <td>2789.649902</td>\n",
       "      <td>7576.359863</td>\n",
       "      <td>1282.000000</td>\n",
       "      <td>56.560001</td>\n",
       "      <td>1128.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>68500</td>\n",
       "      <td>69600</td>\n",
       "      <td>67400</td>\n",
       "      <td>68100</td>\n",
       "      <td>4958303</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>70320.0</td>\n",
       "      <td>0.968430</td>\n",
       "      <td>74160.0</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>2200</td>\n",
       "      <td>1500</td>\n",
       "      <td>700</td>\n",
       "      <td>2771.449951</td>\n",
       "      <td>7505.919922</td>\n",
       "      <td>1284.900024</td>\n",
       "      <td>56.220001</td>\n",
       "      <td>1126.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>67200</td>\n",
       "      <td>68900</td>\n",
       "      <td>67100</td>\n",
       "      <td>68100</td>\n",
       "      <td>3402570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69200.0</td>\n",
       "      <td>0.984104</td>\n",
       "      <td>73770.0</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>2748.929932</td>\n",
       "      <td>7421.459961</td>\n",
       "      <td>1283.800049</td>\n",
       "      <td>56.660000</td>\n",
       "      <td>1127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>67900</td>\n",
       "      <td>68200</td>\n",
       "      <td>66200</td>\n",
       "      <td>66700</td>\n",
       "      <td>3571792</td>\n",
       "      <td>-0.020558</td>\n",
       "      <td>68540.0</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>73270.0</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1200</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>2743.070068</td>\n",
       "      <td>7408.140137</td>\n",
       "      <td>1297.000000</td>\n",
       "      <td>56.070000</td>\n",
       "      <td>1133.069946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close   Volume    Change      5MA   \n",
       "2019-03-04  70300  71400  69900  70100  4931703  0.001429  72880.0  \\\n",
       "2019-03-05  68300  70000  68300  69700  3465521 -0.005706  71740.0   \n",
       "2019-03-06  68500  69600  67400  68100  4958303 -0.022956  70320.0   \n",
       "2019-03-07  67200  68900  67100  68100  3402570  0.000000  69200.0   \n",
       "2019-03-08  67900  68200  66200  66700  3571792 -0.020558  68540.0   \n",
       "\n",
       "            5Disparity     20MA  20Disparity  ...  label   C_O   H_L   H_C   \n",
       "2019-03-04    0.961855  74760.0     0.937667  ...    0.0  -200  1500  1300  \\\n",
       "2019-03-05    0.971564  74450.0     0.936199  ...    0.0  1400  1700   300   \n",
       "2019-03-06    0.968430  74160.0     0.918285  ...    0.0  -400  2200  1500   \n",
       "2019-03-07    0.984104  73770.0     0.923139  ...    0.0   900  1800   800   \n",
       "2019-03-08    0.973154  73270.0     0.910332  ...    0.0 -1200  2000  1500   \n",
       "\n",
       "             C_L      S&P 500       NASDAQ         Gold        Oil   \n",
       "2019-03-04   200  2792.810059  7577.569824  1284.800049  56.590000  \\\n",
       "2019-03-05  1400  2789.649902  7576.359863  1282.000000  56.560001   \n",
       "2019-03-06   700  2771.449951  7505.919922  1284.900024  56.220001   \n",
       "2019-03-07  1000  2748.929932  7421.459961  1283.800049  56.660000   \n",
       "2019-03-08   500  2743.070068  7408.140137  1297.000000  56.070000   \n",
       "\n",
       "                USD/KRW  \n",
       "2019-03-04  1126.810059  \n",
       "2019-03-05  1128.150024  \n",
       "2019-03-06  1126.199951  \n",
       "2019-03-07  1127.000000  \n",
       "2019-03-08  1133.069946  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e9d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_df = data\n",
    "\n",
    "# 상관관계 행렬 계산\n",
    "correlation_matrix = correlation_df.corr()\n",
    "co_df=correlation_matrix[abs(correlation_matrix['Fluctuation'])>0.3][['Fluctuation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de42720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fluctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fluctuation</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Change</th>\n",
       "      <td>0.755341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Individual_trading</th>\n",
       "      <td>-0.591498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_O</th>\n",
       "      <td>0.556812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5Disparity</th>\n",
       "      <td>0.556270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOM</th>\n",
       "      <td>0.502950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign_trading</th>\n",
       "      <td>0.470864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_L</th>\n",
       "      <td>0.464923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>0.422940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SO</th>\n",
       "      <td>0.411648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Williams_R</th>\n",
       "      <td>0.411648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_C</th>\n",
       "      <td>-0.406660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCI</th>\n",
       "      <td>0.405992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Institutional_trading</th>\n",
       "      <td>0.385272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Disparity</th>\n",
       "      <td>0.313764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Fluctuation\n",
       "Fluctuation               1.000000\n",
       "Change                    0.755341\n",
       "Individual_trading       -0.591498\n",
       "C_O                       0.556812\n",
       "5Disparity                0.556270\n",
       "EOM                       0.502950\n",
       "Foreign_trading           0.470864\n",
       "C_L                       0.464923\n",
       "buy                       0.422940\n",
       "SO                        0.411648\n",
       "Williams_R                0.411648\n",
       "H_C                      -0.406660\n",
       "CCI                       0.405992\n",
       "Institutional_trading     0.385272\n",
       "20Disparity               0.313764"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_df.sort_values(by='Fluctuation', key=lambda x: abs(x), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9ec9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature           VIF\n",
      "0                    Open           inf\n",
      "1                    High           inf\n",
      "2                     Low           inf\n",
      "3                   Close           inf\n",
      "4                  Volume  3.426427e+00\n",
      "5                  Change  8.219011e+00\n",
      "6                     5MA  7.991741e+03\n",
      "7              5Disparity  9.884346e+01\n",
      "8                    20MA           inf\n",
      "9             20Disparity  1.748138e+02\n",
      "10                   60MA  4.115100e+03\n",
      "11            60Disparity  7.546547e+02\n",
      "12                  120MA  4.746741e+02\n",
      "13           120Disparity  2.223457e+02\n",
      "14               bol_high           inf\n",
      "15                bol_low           inf\n",
      "16                    PPO  1.130640e+03\n",
      "17                    PVO  2.701351e+00\n",
      "18                     SO  2.467813e+04\n",
      "19             Williams_R  2.282126e+04\n",
      "20                    EOM  3.197742e+00\n",
      "21                    MFI  3.850879e+00\n",
      "22                    NVI  2.589774e+00\n",
      "23                    OBV  2.956321e+01\n",
      "24                    ATR  1.121372e+01\n",
      "25                     UI  1.386749e+01\n",
      "26                    ADX  3.804722e+00\n",
      "27                   MACD  7.038843e+02\n",
      "28                   TRIX  4.786008e+02\n",
      "29                    CCI  9.258234e+00\n",
      "30                    RMI  7.711355e+00\n",
      "31                    VHF  1.492318e+00\n",
      "32  Institutional_trading  2.736399e+02\n",
      "33     Individual_trading  1.059036e+03\n",
      "34        Foreign_trading  7.457743e+02\n",
      "35           Short_volume  1.464597e+00\n",
      "36             Short_left  4.891446e+00\n",
      "37                Cos_sim  1.096815e+00\n",
      "38                    wil  2.274812e+03\n",
      "39                    buy  2.444723e+00\n",
      "40                  label  1.250099e+00\n",
      "41                    C_O           inf\n",
      "42                    H_L           inf\n",
      "43                    H_C           inf\n",
      "44                    C_L           inf\n",
      "45                S&P 500  9.639238e+01\n",
      "46                 NASDAQ  7.404552e+01\n",
      "47                   Gold  4.136401e+00\n",
      "48                    Oil  5.461604e+00\n",
      "49                USD/KRW  8.225445e+00\n",
      "         feature         VIF\n",
      "0         Volume   11.136148\n",
      "1         Change    2.379652\n",
      "2            PVO    2.108362\n",
      "3            EOM    2.052840\n",
      "4            MFI   27.222296\n",
      "5            NVI   64.951590\n",
      "6            ADX    7.803115\n",
      "7            CCI    2.485458\n",
      "8            RMI    7.953910\n",
      "9            VHF   16.016456\n",
      "10  Short_volume    1.527860\n",
      "11    Short_left    5.595616\n",
      "12       Cos_sim    2.563897\n",
      "13           buy    2.147436\n",
      "14         label    1.389159\n",
      "15          Gold  105.414422\n",
      "16           Oil   16.840092\n",
      "17       USD/KRW  212.994198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           Volume\n",
       "1           Change\n",
       "2              PVO\n",
       "3              EOM\n",
       "4              MFI\n",
       "5              NVI\n",
       "6              ADX\n",
       "7              CCI\n",
       "8              RMI\n",
       "9              VHF\n",
       "10    Short_volume\n",
       "11      Short_left\n",
       "12         Cos_sim\n",
       "13             buy\n",
       "14           label\n",
       "15            Gold\n",
       "16             Oil\n",
       "17         USD/KRW\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다중공선성 확인 \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "stock_data = data\n",
    "\n",
    "X = stock_data.drop(columns=['Fluctuation'])  \n",
    "y = stock_data['Fluctuation']\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif = calculate_vif(X)\n",
    "print(vif)\n",
    "\n",
    "# 다중공선성이 높은 변수를 제거. VIF가 10 이상인 변수는 다중공선성이 있는 것으로 판단\n",
    "high_vif_variables = vif[vif['VIF'] > 10]['feature']\n",
    "X_filtered = X.drop(columns=high_vif_variables)\n",
    "\n",
    "vif_filtered = calculate_vif(X_filtered)\n",
    "print(vif_filtered)\n",
    "vif_filtered['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c26e628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CCI</td>\n",
       "      <td>9.258234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>USD/KRW</td>\n",
       "      <td>8.225445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Change</td>\n",
       "      <td>8.219011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RMI</td>\n",
       "      <td>7.711355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Oil</td>\n",
       "      <td>5.461604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Short_left</td>\n",
       "      <td>4.891446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Gold</td>\n",
       "      <td>4.136401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MFI</td>\n",
       "      <td>3.850879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ADX</td>\n",
       "      <td>3.804722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Volume</td>\n",
       "      <td>3.426427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EOM</td>\n",
       "      <td>3.197742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PVO</td>\n",
       "      <td>2.701351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NVI</td>\n",
       "      <td>2.589774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>buy</td>\n",
       "      <td>2.444723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VHF</td>\n",
       "      <td>1.492318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Short_volume</td>\n",
       "      <td>1.464597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>label</td>\n",
       "      <td>1.250099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Cos_sim</td>\n",
       "      <td>1.096815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature       VIF\n",
       "29           CCI  9.258234\n",
       "49       USD/KRW  8.225445\n",
       "5         Change  8.219011\n",
       "30           RMI  7.711355\n",
       "48           Oil  5.461604\n",
       "36    Short_left  4.891446\n",
       "47          Gold  4.136401\n",
       "21           MFI  3.850879\n",
       "26           ADX  3.804722\n",
       "4         Volume  3.426427\n",
       "20           EOM  3.197742\n",
       "17           PVO  2.701351\n",
       "22           NVI  2.589774\n",
       "39           buy  2.444723\n",
       "31           VHF  1.492318\n",
       "35  Short_volume  1.464597\n",
       "40         label  1.250099\n",
       "37       Cos_sim  1.096815"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vif[vif['VIF'] < 10].sort_values('VIF',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b2ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Change</td>\n",
       "      <td>0.400434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Individual_trading</td>\n",
       "      <td>0.144462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5Disparity</td>\n",
       "      <td>0.056587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C_O</td>\n",
       "      <td>0.055626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EOM</td>\n",
       "      <td>0.046843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Foreign_trading</td>\n",
       "      <td>0.043129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>C_L</td>\n",
       "      <td>0.034077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>buy</td>\n",
       "      <td>0.018284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Institutional_trading</td>\n",
       "      <td>0.017279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>H_C</td>\n",
       "      <td>0.015265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature  Importance\n",
       "5                  Change    0.400434\n",
       "33     Individual_trading    0.144462\n",
       "7              5Disparity    0.056587\n",
       "41                    C_O    0.055626\n",
       "20                    EOM    0.046843\n",
       "34        Foreign_trading    0.043129\n",
       "44                    C_L    0.034077\n",
       "39                    buy    0.018284\n",
       "32  Institutional_trading    0.017279\n",
       "43                    H_C    0.015265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "df=data.copy()\n",
    "\n",
    "X = df.drop(columns=['Fluctuation'])  \n",
    "y = df['Fluctuation']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# 상위 중요도를 가진 feature 선택\n",
    "top_features = feature_importance_df.head(10)\n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d66bd3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40043377, 0.14446245, 0.05658738, 0.05562617, 0.04684287,\n",
       "       0.04312918, 0.03407699, 0.01828432, 0.01727879, 0.01526546,\n",
       "       0.01474567, 0.01459178, 0.01357798, 0.00660676, 0.00630647,\n",
       "       0.00568865, 0.00508391, 0.00444453, 0.00435228, 0.00399817])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features['Importance'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3697f8",
   "metadata": {},
   "source": [
    "### 특성별 변수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8027db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>C_O</th>\n",
       "      <th>H_L</th>\n",
       "      <th>H_C</th>\n",
       "      <th>C_L</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>70300</td>\n",
       "      <td>71400</td>\n",
       "      <td>69900</td>\n",
       "      <td>70100</td>\n",
       "      <td>4931703</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>72880.0</td>\n",
       "      <td>0.961855</td>\n",
       "      <td>74760.0</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>200</td>\n",
       "      <td>2792.810059</td>\n",
       "      <td>7577.569824</td>\n",
       "      <td>1284.800049</td>\n",
       "      <td>56.590000</td>\n",
       "      <td>1126.810059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>68300</td>\n",
       "      <td>70000</td>\n",
       "      <td>68300</td>\n",
       "      <td>69700</td>\n",
       "      <td>3465521</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>71740.0</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1700</td>\n",
       "      <td>300</td>\n",
       "      <td>1400</td>\n",
       "      <td>2789.649902</td>\n",
       "      <td>7576.359863</td>\n",
       "      <td>1282.000000</td>\n",
       "      <td>56.560001</td>\n",
       "      <td>1128.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>68500</td>\n",
       "      <td>69600</td>\n",
       "      <td>67400</td>\n",
       "      <td>68100</td>\n",
       "      <td>4958303</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>70320.0</td>\n",
       "      <td>0.968430</td>\n",
       "      <td>74160.0</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>2200</td>\n",
       "      <td>1500</td>\n",
       "      <td>700</td>\n",
       "      <td>2771.449951</td>\n",
       "      <td>7505.919922</td>\n",
       "      <td>1284.900024</td>\n",
       "      <td>56.220001</td>\n",
       "      <td>1126.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>67200</td>\n",
       "      <td>68900</td>\n",
       "      <td>67100</td>\n",
       "      <td>68100</td>\n",
       "      <td>3402570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69200.0</td>\n",
       "      <td>0.984104</td>\n",
       "      <td>73770.0</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>2748.929932</td>\n",
       "      <td>7421.459961</td>\n",
       "      <td>1283.800049</td>\n",
       "      <td>56.660000</td>\n",
       "      <td>1127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>67900</td>\n",
       "      <td>68200</td>\n",
       "      <td>66200</td>\n",
       "      <td>66700</td>\n",
       "      <td>3571792</td>\n",
       "      <td>-0.020558</td>\n",
       "      <td>68540.0</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>73270.0</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1200</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>2743.070068</td>\n",
       "      <td>7408.140137</td>\n",
       "      <td>1297.000000</td>\n",
       "      <td>56.070000</td>\n",
       "      <td>1133.069946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close   Volume    Change      5MA   \n",
       "2019-03-04  70300  71400  69900  70100  4931703  0.001429  72880.0  \\\n",
       "2019-03-05  68300  70000  68300  69700  3465521 -0.005706  71740.0   \n",
       "2019-03-06  68500  69600  67400  68100  4958303 -0.022956  70320.0   \n",
       "2019-03-07  67200  68900  67100  68100  3402570  0.000000  69200.0   \n",
       "2019-03-08  67900  68200  66200  66700  3571792 -0.020558  68540.0   \n",
       "\n",
       "            5Disparity     20MA  20Disparity  ...  label   C_O   H_L   H_C   \n",
       "2019-03-04    0.961855  74760.0     0.937667  ...    0.0  -200  1500  1300  \\\n",
       "2019-03-05    0.971564  74450.0     0.936199  ...    0.0  1400  1700   300   \n",
       "2019-03-06    0.968430  74160.0     0.918285  ...    0.0  -400  2200  1500   \n",
       "2019-03-07    0.984104  73770.0     0.923139  ...    0.0   900  1800   800   \n",
       "2019-03-08    0.973154  73270.0     0.910332  ...    0.0 -1200  2000  1500   \n",
       "\n",
       "             C_L      S&P 500       NASDAQ         Gold        Oil   \n",
       "2019-03-04   200  2792.810059  7577.569824  1284.800049  56.590000  \\\n",
       "2019-03-05  1400  2789.649902  7576.359863  1282.000000  56.560001   \n",
       "2019-03-06   700  2771.449951  7505.919922  1284.900024  56.220001   \n",
       "2019-03-07  1000  2748.929932  7421.459961  1283.800049  56.660000   \n",
       "2019-03-08   500  2743.070068  7408.140137  1297.000000  56.070000   \n",
       "\n",
       "                USD/KRW  \n",
       "2019-03-04  1126.810059  \n",
       "2019-03-05  1128.150024  \n",
       "2019-03-06  1126.199951  \n",
       "2019-03-07  1127.000000  \n",
       "2019-03-08  1133.069946  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_df_filled.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45384bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=correlation_matrix[abs(correlation_matrix['Fluctuation'])>0.3][['Fluctuation']].index.to_list()\n",
    "b=vif[vif['VIF']<10]['feature'].tolist()\n",
    "lis=list(set(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b75d05f0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RMI',\n",
       " 'SO',\n",
       " 'Gold',\n",
       " 'ADX',\n",
       " 'Foreign_trading',\n",
       " 'Oil',\n",
       " 'Volume',\n",
       " 'MFI',\n",
       " 'USD/KRW',\n",
       " 'Williams_R',\n",
       " 'Institutional_trading',\n",
       " 'label',\n",
       " 'C_L',\n",
       " 'VHF',\n",
       " 'EOM',\n",
       " 'PVO',\n",
       " 'Individual_trading',\n",
       " 'NVI',\n",
       " 'H_C',\n",
       " 'Change',\n",
       " 'buy',\n",
       " 'Short_left',\n",
       " 'Fluctuation',\n",
       " '20Disparity',\n",
       " '5Disparity',\n",
       " 'CCI',\n",
       " 'Cos_sim',\n",
       " 'Short_volume',\n",
       " 'C_O']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e3dbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균이상도 관련 지표 \n",
    "moving_average_disparity_related_indicators = ['5Disparity','20Disparity']\n",
    "\n",
    "# 주식 거래자 유형 관련 지표 \n",
    "trader_type_related_indicators = ['Institutional_trading', 'Individual_trading', 'Foreign_trading']\n",
    "\n",
    "# 외부 재무지표\n",
    "eco_indicators=['Gold','Oil','USD/KRW']\n",
    "\n",
    "# 가격 관련 \n",
    "trade_indicators=['C_O','H_C','C_L','Short_left', 'Short_volume']\n",
    "\n",
    "# 추세 추종 지표\n",
    "trend_indicators = ['ADX', 'CCI','VHF']\n",
    "\n",
    "# 오실레이터 지표 \n",
    "oscillator_indicators = ['RMI', 'SO', 'Williams_R']\n",
    "\n",
    "# 거래량 관련 지표\n",
    "volume_indicators = ['MFI', 'EOM', 'PVO', 'NVI']\n",
    "\n",
    "# 기사 감성분석 \n",
    "news_indicators=['label']\n",
    "\n",
    "# 코사인 유사도 \n",
    "cos_indicators=['Cos_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f060faa0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3905975588.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[122], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    combined_data          0.540164\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " combined_data          0.540164\n",
    "1  combined_data2          0.513525\n",
    "2  combined_data3          0.531148\n",
    "3  combined_data4          0.522541\n",
    "4  combined_data5          0.477459\n",
    "5  combined_data6          0.590164\n",
    "6  combined_data7          0.506967\n",
    "7  combined_data8          0.554918\n",
    "8  combined_data9          0.534836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59c3e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=data[['Open', 'High', 'Low', 'Close', 'Change','Fluctuation','Volume']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5b3af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "combined_data = pd.concat([data[moving_average_disparity_related_indicators], new_data], axis=1) #0.54\n",
    "combined_data2 = pd.concat([data[trader_type_related_indicators], new_data], axis=1) #515\n",
    "combined_data3 = pd.concat([data[eco_indicators], new_data], axis=1)# 53\n",
    "combined_data4 = pd.concat([data[trade_indicators], new_data], axis=1) # 534\n",
    "\n",
    "combined_data5 = pd.concat([data[trend_indicators], new_data], axis=1) # 49\n",
    "combined_data6=pd.concat([data[oscillator_indicators], new_data], axis=1) # 5\n",
    "combined_data7=pd.concat([data[volume_indicators], new_data], axis=1) # 52\n",
    "combined_data8=pd.concat([data[news_indicators], new_data], axis=1)# 554\n",
    "combined_data9=pd.concat([data[cos_indicators], new_data], axis=1)  # 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e87fee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>C_O</th>\n",
       "      <th>H_L</th>\n",
       "      <th>H_C</th>\n",
       "      <th>C_L</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>70300</td>\n",
       "      <td>71400</td>\n",
       "      <td>69900</td>\n",
       "      <td>70100</td>\n",
       "      <td>4931703</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>72880.0</td>\n",
       "      <td>0.961855</td>\n",
       "      <td>74760.0</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>200</td>\n",
       "      <td>2792.810059</td>\n",
       "      <td>7577.569824</td>\n",
       "      <td>1284.800049</td>\n",
       "      <td>56.590000</td>\n",
       "      <td>1126.810059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>68300</td>\n",
       "      <td>70000</td>\n",
       "      <td>68300</td>\n",
       "      <td>69700</td>\n",
       "      <td>3465521</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>71740.0</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>74450.0</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1700</td>\n",
       "      <td>300</td>\n",
       "      <td>1400</td>\n",
       "      <td>2789.649902</td>\n",
       "      <td>7576.359863</td>\n",
       "      <td>1282.000000</td>\n",
       "      <td>56.560001</td>\n",
       "      <td>1128.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>68500</td>\n",
       "      <td>69600</td>\n",
       "      <td>67400</td>\n",
       "      <td>68100</td>\n",
       "      <td>4958303</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>70320.0</td>\n",
       "      <td>0.968430</td>\n",
       "      <td>74160.0</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400</td>\n",
       "      <td>2200</td>\n",
       "      <td>1500</td>\n",
       "      <td>700</td>\n",
       "      <td>2771.449951</td>\n",
       "      <td>7505.919922</td>\n",
       "      <td>1284.900024</td>\n",
       "      <td>56.220001</td>\n",
       "      <td>1126.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>67200</td>\n",
       "      <td>68900</td>\n",
       "      <td>67100</td>\n",
       "      <td>68100</td>\n",
       "      <td>3402570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69200.0</td>\n",
       "      <td>0.984104</td>\n",
       "      <td>73770.0</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>2748.929932</td>\n",
       "      <td>7421.459961</td>\n",
       "      <td>1283.800049</td>\n",
       "      <td>56.660000</td>\n",
       "      <td>1127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>67900</td>\n",
       "      <td>68200</td>\n",
       "      <td>66200</td>\n",
       "      <td>66700</td>\n",
       "      <td>3571792</td>\n",
       "      <td>-0.020558</td>\n",
       "      <td>68540.0</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>73270.0</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1200</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>2743.070068</td>\n",
       "      <td>7408.140137</td>\n",
       "      <td>1297.000000</td>\n",
       "      <td>56.070000</td>\n",
       "      <td>1133.069946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close   Volume    Change      5MA   \n",
       "2019-03-04  70300  71400  69900  70100  4931703  0.001429  72880.0  \\\n",
       "2019-03-05  68300  70000  68300  69700  3465521 -0.005706  71740.0   \n",
       "2019-03-06  68500  69600  67400  68100  4958303 -0.022956  70320.0   \n",
       "2019-03-07  67200  68900  67100  68100  3402570  0.000000  69200.0   \n",
       "2019-03-08  67900  68200  66200  66700  3571792 -0.020558  68540.0   \n",
       "\n",
       "            5Disparity     20MA  20Disparity  ...  label   C_O   H_L   H_C   \n",
       "2019-03-04    0.961855  74760.0     0.937667  ...    0.0  -200  1500  1300  \\\n",
       "2019-03-05    0.971564  74450.0     0.936199  ...    0.0  1400  1700   300   \n",
       "2019-03-06    0.968430  74160.0     0.918285  ...    0.0  -400  2200  1500   \n",
       "2019-03-07    0.984104  73770.0     0.923139  ...    0.0   900  1800   800   \n",
       "2019-03-08    0.973154  73270.0     0.910332  ...    0.0 -1200  2000  1500   \n",
       "\n",
       "             C_L      S&P 500       NASDAQ         Gold        Oil   \n",
       "2019-03-04   200  2792.810059  7577.569824  1284.800049  56.590000  \\\n",
       "2019-03-05  1400  2789.649902  7576.359863  1282.000000  56.560001   \n",
       "2019-03-06   700  2771.449951  7505.919922  1284.900024  56.220001   \n",
       "2019-03-07  1000  2748.929932  7421.459961  1283.800049  56.660000   \n",
       "2019-03-08   500  2743.070068  7408.140137  1297.000000  56.070000   \n",
       "\n",
       "                USD/KRW  \n",
       "2019-03-04  1126.810059  \n",
       "2019-03-05  1128.150024  \n",
       "2019-03-06  1126.199951  \n",
       "2019-03-07  1127.000000  \n",
       "2019-03-08  1133.069946  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_df_filled.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46f9b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=moving_average_disparity_related_indicators+news_indicators+trade_indicators+cos_indicators #0.5368\n",
    "# c=moving_average_disparity_related_indicators+news_indicators+cos_indicators+  eco_indicators+ trade_indicators\n",
    "\n",
    "data=pd.concat([new_data,data[c]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "33d19aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Change', 'Fluctuation', 'Volume',\n",
       "       '5Disparity', '20Disparity', 'label', 'C_O', 'H_C', 'C_L', 'Short_left',\n",
       "       'Short_volume', 'Cos_sim'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "# combined_data2,combined_data5,combined_data6,combined_data7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8b3d6",
   "metadata": {},
   "source": [
    "## 모델링 \n",
    "- 5년치 데이터 최적의 파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5cc6956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Fluctuation\"\n",
    "SEQ_SIZE = 30        # 30 / 60 / 120\n",
    "PRED_SIZE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f40d18ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.03000000e+04  7.14000000e+04  6.99000000e+04  7.01000000e+04\n",
      "   1.42857143e-03  1.00000000e+00  4.93170300e+06  9.61855104e-01\n",
      "   9.37667202e-01  0.00000000e+00 -2.00000000e+02  1.30000000e+03\n",
      "   2.00000000e+02  1.85614700e+06  3.86605000e+05  0.00000000e+00]\n",
      " [ 6.83000000e+04  7.00000000e+04  6.83000000e+04  6.97000000e+04\n",
      "  -5.70613409e-03  0.00000000e+00  3.46552100e+06  9.71563981e-01\n",
      "   9.36198791e-01  0.00000000e+00  1.40000000e+03  3.00000000e+02\n",
      "   1.40000000e+03  1.65942100e+06  2.66833000e+05  0.00000000e+00]\n",
      " [ 6.85000000e+04  6.96000000e+04  6.74000000e+04  6.81000000e+04\n",
      "  -2.29555237e-02  0.00000000e+00  4.95830300e+06  9.68430034e-01\n",
      "   9.18284790e-01  0.00000000e+00 -4.00000000e+02  1.50000000e+03\n",
      "   7.00000000e+02  1.33748700e+06  6.53245000e+05  1.00000000e+00]\n",
      " [ 6.72000000e+04  6.89000000e+04  6.71000000e+04  6.81000000e+04\n",
      "   0.00000000e+00  0.00000000e+00  3.40257000e+06  9.84104046e-01\n",
      "   9.23139488e-01  0.00000000e+00  9.00000000e+02  8.00000000e+02\n",
      "   1.00000000e+03  1.16000300e+06  2.27079000e+05  0.00000000e+00]\n",
      " [ 6.79000000e+04  6.82000000e+04  6.62000000e+04  6.67000000e+04\n",
      "  -2.05580029e-02  0.00000000e+00  3.57179200e+06  9.73154362e-01\n",
      "   9.10331650e-01  0.00000000e+00 -1.20000000e+03  1.50000000e+03\n",
      "   5.00000000e+02  1.18842500e+06  4.41254000e+05  0.00000000e+00]\n",
      " [ 6.70000000e+04  6.78000000e+04  6.62000000e+04  6.66000000e+04\n",
      "  -1.49925037e-03  0.00000000e+00  2.86900800e+06  9.81721698e-01\n",
      "   9.13267055e-01  0.00000000e+00 -4.00000000e+02  1.20000000e+03\n",
      "   4.00000000e+02  1.17351300e+06  2.05243000e+05  1.00000000e+00]\n",
      " [ 6.77000000e+04  6.86000000e+04  6.75000000e+04  6.77000000e+04\n",
      "   1.65165165e-02  1.00000000e+00  2.75353500e+06  1.00385528e+00\n",
      "   9.32442669e-01  1.00000000e+00  0.00000000e+00  9.00000000e+02\n",
      "   2.00000000e+02  1.07948300e+06  3.96350000e+04  0.00000000e+00]\n",
      " [ 6.77000000e+04  6.78000000e+04  6.59000000e+04  6.68000000e+04\n",
      "  -1.32939439e-02  0.00000000e+00  2.85159600e+06  9.94343555e-01\n",
      "   9.25848926e-01  1.00000000e+00 -9.00000000e+02  1.00000000e+03\n",
      "   9.00000000e+02  1.07081200e+06  1.56567000e+05  1.00000000e+00]\n",
      " [ 6.74000000e+04  6.74000000e+04  6.64000000e+04  6.73000000e+04\n",
      "   7.48502994e-03  1.00000000e+00  3.10710000e+06  1.00417786e+00\n",
      "   9.38567743e-01  1.00000000e+00 -1.00000000e+02  1.00000000e+02\n",
      "   9.00000000e+02  1.09676700e+06  3.24211000e+05  1.00000000e+00]\n",
      " [ 6.74000000e+04  6.87000000e+04  6.67000000e+04  6.81000000e+04\n",
      "   1.18870728e-02  1.00000000e+00  3.64559200e+06  1.01188707e+00\n",
      "   9.55923638e-01  5.45454545e-01  7.00000000e+02  6.00000000e+02\n",
      "   1.40000000e+03  1.03363700e+06  7.41230000e+04  1.00000000e+00]\n",
      " [ 6.92000000e+04  6.93000000e+04  6.73000000e+04  6.78000000e+04\n",
      "  -4.40528634e-03  0.00000000e+00  2.15858800e+06  1.00384957e+00\n",
      "   9.55737243e-01 -2.00000000e-01 -1.40000000e+03  1.50000000e+03\n",
      "   5.00000000e+02  1.07658900e+06  1.36947000e+05  1.00000000e+00]\n",
      " [ 6.73000000e+04  6.80000000e+04  6.66000000e+04  6.80000000e+04\n",
      "   2.94985251e-03  1.00000000e+00  2.28516900e+06  1.00591716e+00\n",
      "   9.63104596e-01 -3.63636364e-01  7.00000000e+02  0.00000000e+00\n",
      "   1.40000000e+03  1.06307600e+06  1.30097000e+05  1.00000000e+00]\n",
      " [ 6.84000000e+04  7.06000000e+04  6.76000000e+04  7.05000000e+04\n",
      "   3.67647059e-02  1.00000000e+00  3.64296200e+06  1.03160667e+00\n",
      "   1.00078075e+00  0.00000000e+00  2.10000000e+03  1.00000000e+02\n",
      "   2.90000000e+03  1.07153700e+06  3.21739000e+05  1.00000000e+00]\n",
      " [ 7.35000000e+04  7.60000000e+04  7.26000000e+04  7.59000000e+04\n",
      "   7.65957447e-02  1.00000000e+00  1.00301310e+07  1.08335712e+00\n",
      "   1.07728337e+00  4.61538462e-01  2.40000000e+03  1.00000000e+02\n",
      "   3.30000000e+03  9.84604000e+05  8.18723000e+05  1.00000000e+00]\n",
      " [ 7.89000000e+04  7.89000000e+04  7.59000000e+04  7.61000000e+04\n",
      "   2.63504611e-03  1.00000000e+00  5.28645500e+06  1.06195925e+00\n",
      "   1.08058218e+00 -4.65116279e-02 -2.80000000e+03  2.80000000e+03\n",
      "   2.00000000e+02  1.16146200e+06  3.88497000e+05  1.00000000e+00]\n",
      " [ 7.37000000e+04  7.42000000e+04  7.26000000e+04  7.29000000e+04\n",
      "  -4.20499343e-02  0.00000000e+00  3.75223000e+06  1.00302697e+00\n",
      "   1.03794404e+00  2.30769231e-01 -8.00000000e+02  1.30000000e+03\n",
      "   3.00000000e+02  1.27471800e+06  4.64822000e+05  0.00000000e+00]\n",
      " [ 7.29000000e+04  7.33000000e+04  7.15000000e+04  7.27000000e+04\n",
      "  -2.74348422e-03  0.00000000e+00  2.98767800e+06  9.87503396e-01\n",
      "   1.03708987e+00 -8.75000000e-01 -2.00000000e+02  6.00000000e+02\n",
      "   1.20000000e+03  1.27646400e+06  3.69602000e+05  0.00000000e+00]\n",
      " [ 7.18000000e+04  7.41000000e+04  7.12000000e+04  7.34000000e+04\n",
      "   9.62861073e-03  1.00000000e+00  2.74417500e+06  9.89218329e-01\n",
      "   1.04842165e+00  3.22916667e-01  1.60000000e+03  7.00000000e+02\n",
      "   2.20000000e+03  1.26315200e+06  1.68508000e+05  1.00000000e+00]\n",
      " [ 7.23000000e+04  7.36000000e+04  7.17000000e+04  7.23000000e+04\n",
      "  -1.49863760e-02  0.00000000e+00  2.77493200e+06  9.83941208e-01\n",
      "   1.03374321e+00  1.37931034e-01  0.00000000e+00  1.30000000e+03\n",
      "   6.00000000e+02  1.22089600e+06  1.84712000e+05  0.00000000e+00]\n",
      " [ 7.29000000e+04  7.49000000e+04  7.27000000e+04  7.42000000e+04\n",
      "   2.62793914e-02  1.00000000e+00  3.21417000e+06  1.01504788e+00\n",
      "   1.05773343e+00  4.00000000e-01  1.30000000e+03  7.00000000e+02\n",
      "   1.50000000e+03  1.23503200e+06  1.69883000e+05  1.00000000e+00]\n",
      " [ 7.56000000e+04  7.71000000e+04  7.55000000e+04  7.66000000e+04\n",
      "   3.23450135e-02  1.00000000e+00  3.57907000e+06  1.03737811e+00\n",
      "   1.08691025e+00  0.00000000e+00  1.00000000e+03  5.00000000e+02\n",
      "   1.10000000e+03  1.26569400e+06  2.00976000e+05  1.00000000e+00]\n",
      " [ 7.79000000e+04  7.80000000e+04  7.61000000e+04  7.64000000e+04\n",
      "  -2.61096606e-03  0.00000000e+00  2.71944000e+06  1.02440333e+00\n",
      "   1.07894365e+00  0.00000000e+00 -1.50000000e+03  1.60000000e+03\n",
      "   3.00000000e+02  1.34094500e+06  1.56510000e+05  1.00000000e+00]\n",
      " [ 7.68000000e+04  7.99000000e+04  7.65000000e+04  7.99000000e+04\n",
      "   4.58115183e-02  1.00000000e+00  4.67414200e+06  1.05297839e+00\n",
      "   1.11904762e+00  0.00000000e+00  3.10000000e+03  0.00000000e+00\n",
      "   3.40000000e+03  1.22974900e+06  1.94354000e+05  1.00000000e+00]\n",
      " [ 8.04000000e+04  8.14000000e+04  7.78000000e+04  7.84000000e+04\n",
      "  -1.87734668e-02  0.00000000e+00  5.43623600e+06  1.01686122e+00\n",
      "   1.09017590e+00  0.00000000e+00 -2.00000000e+03  3.00000000e+03\n",
      "   6.00000000e+02  1.17426300e+06  2.97970000e+05  1.00000000e+00]\n",
      " [ 7.82000000e+04  7.93000000e+04  7.79000000e+04  7.90000000e+04\n",
      "   7.65306122e-03  1.00000000e+00  2.19172000e+06  1.01204202e+00\n",
      "   1.08920447e+00  0.00000000e+00  8.00000000e+02  3.00000000e+02\n",
      "   1.10000000e+03  1.13273400e+06  9.72490000e+04  0.00000000e+00]\n",
      " [ 7.89000000e+04  7.92000000e+04  7.76000000e+04  7.79000000e+04\n",
      "  -1.39240506e-02  0.00000000e+00  2.66349300e+06  9.94637385e-01\n",
      "   1.06573637e+00  1.00000000e+00 -1.00000000e+03  1.30000000e+03\n",
      "   3.00000000e+02  1.04250100e+06  1.30946000e+05  1.00000000e+00]\n",
      " [ 7.77000000e+04  7.87000000e+04  7.74000000e+04  7.78000000e+04\n",
      "  -1.28369705e-03  0.00000000e+00  2.32286000e+06  9.89821883e-01\n",
      "   1.05706522e+00  1.00000000e+00  1.00000000e+02  9.00000000e+02\n",
      "   4.00000000e+02  9.73398000e+05  6.38460000e+04  0.00000000e+00]\n",
      " [ 7.67000000e+04  7.88000000e+04  7.62000000e+04  7.86000000e+04\n",
      "   1.02827763e-02  1.00000000e+00  2.68959100e+06  1.00331887e+00\n",
      "   1.05944197e+00  1.00000000e+00  1.90000000e+03  2.00000000e+02\n",
      "   2.40000000e+03  8.16863000e+05  2.63770000e+05  1.00000000e+00]\n",
      " [ 7.92000000e+04  7.95000000e+04  7.80000000e+04  7.84000000e+04\n",
      "  -2.54452926e-03  0.00000000e+00  2.61438800e+06  1.00076589e+00\n",
      "   1.04889959e+00  5.00000000e-01 -8.00000000e+02  1.10000000e+03\n",
      "   4.00000000e+02  8.91913000e+05  3.07568000e+05  0.00000000e+00]\n",
      " [ 7.74000000e+04  7.88000000e+04  7.74000000e+04  7.81000000e+04\n",
      "  -3.82653061e-03  0.00000000e+00  2.31726400e+06  9.99232344e-01\n",
      "   1.03794272e+00  5.00000000e-01  7.00000000e+02  7.00000000e+02\n",
      "   7.00000000e+02  1.03134900e+06  3.49878000e+05  1.00000000e+00]] \n",
      " [1]\n",
      "X size :  (1204, 30, 16)\n",
      "y size :  (1204, 1)\n"
     ]
    }
   ],
   "source": [
    "def split_xy(dataset, time_steps, y_column):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i + time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "\n",
    "        if y_end_number > len(dataset):\n",
    "            break\n",
    "        tmp_x = dataset.iloc[i:x_end_number, :]  # Adjusted for Pandas\n",
    "        tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "        x.append(tmp_x.values)  # Convert to numpy array\n",
    "        y.append(tmp_y.values)  # Convert to numpy array\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "X, y = split_xy(data, SEQ_SIZE, PRED_SIZE)\n",
    "print(X[0],\"\\n\", y[0])\n",
    "print(\"X size : \", X.shape)\n",
    "print(\"y size : \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca44b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 표준졍규화-> 시퀀스가 작을 때 표준편차가 0이 되는 경우가 있어서 이를 확인하고 선택해야 함 \n",
    "import numpy as np\n",
    "# 시퀀스별로 정규화\n",
    "def scale_sequences(data):\n",
    "    scaled_data = []\n",
    "    for sample in data:\n",
    "        # 시퀀스별로 평균과 표준 편차 계산\n",
    "        mean = np.mean(sample, axis=0)\n",
    "        std = np.std(sample, axis=0)\n",
    "        if 0 in std:\n",
    "            print('error')\n",
    "        # 표준화 수행\n",
    "        scaled_sample = (sample - mean) / std\n",
    "        scaled_data.append(scaled_sample)\n",
    "    return np.array(scaled_data)\n",
    "\n",
    "# 주식 데이터를 시퀀스별로 표준화\n",
    "X = scale_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3d11972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204, 30, 16)\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스별로 Min-Max 정규화\n",
    "def min_max_scale_sequences(data):\n",
    "    scaled_data = np.zeros_like(data) \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[2]):\n",
    "            min_val = np.min(data[i, :, j])\n",
    "            max_val = np.max(data[i, :, j])\n",
    "            if min_val == max_val:\n",
    "                scaled_data[i, :, j] = 0\n",
    "            else:\n",
    "                scaled_data[i, :, j] = (data[i, :, j] - min_val) / (max_val - min_val)\n",
    "    return scaled_data\n",
    "\n",
    "# 주식 데이터를 시퀀스별로 Min-Max 정규화\n",
    "X_normalized = min_max_scale_sequences(X)\n",
    "\n",
    "# 결과 확인\n",
    "print(X_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81e61c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 3차원 배열 데이터를 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "396a8c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: early stopping\n",
      "Restoring model weights from the end of the best epoch: 65.\n",
      "Epoch 57: early stopping\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 68: early stopping\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 70: early stopping\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 62: early stopping\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 68: early stopping\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "Epoch 65: early stopping\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "Epoch 67: early stopping\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 62: early stopping\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 67: early stopping\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "평균 정확도: 0.5188524603843689\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# 초기화 함수 선택\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# 정확도를 저장할 리스트\n",
    "accuracies = []\n",
    "\n",
    "# 10번의 반복 훈련\n",
    "for _ in range(10):\n",
    "    # 모델 구성\n",
    "    model = Sequential([\n",
    "        GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.003)),\n",
    "        Dense(32, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.003)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=8, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # 테스트 데이터로 평가\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# 정확도 평균 계산\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f'평균 정확도: {mean_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc4b59",
   "metadata": {},
   "source": [
    "### 5년치 최적 변수의 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "323b5fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.4549 - loss: 1.0005 - val_accuracy: 0.5492 - val_loss: 0.9793\n",
      "Epoch 2/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5333 - loss: 0.9741 - val_accuracy: 0.5337 - val_loss: 0.9587\n",
      "Epoch 3/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5346 - loss: 0.9530 - val_accuracy: 0.5285 - val_loss: 0.9415\n",
      "Epoch 4/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5300 - loss: 0.9371 - val_accuracy: 0.5181 - val_loss: 0.9253\n",
      "Epoch 5/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5329 - loss: 0.9197 - val_accuracy: 0.5181 - val_loss: 0.9101\n",
      "Epoch 6/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5520 - loss: 0.9032 - val_accuracy: 0.5233 - val_loss: 0.8959\n",
      "Epoch 7/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5317 - loss: 0.8908 - val_accuracy: 0.5181 - val_loss: 0.8831\n",
      "Epoch 8/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5367 - loss: 0.8780 - val_accuracy: 0.5181 - val_loss: 0.8714\n",
      "Epoch 9/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5242 - loss: 0.8672 - val_accuracy: 0.5181 - val_loss: 0.8600\n",
      "Epoch 10/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4781 - loss: 0.8620 - val_accuracy: 0.5181 - val_loss: 0.8499\n",
      "Epoch 11/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5148 - loss: 0.8468 - val_accuracy: 0.5181 - val_loss: 0.8402\n",
      "Epoch 12/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5306 - loss: 0.8343 - val_accuracy: 0.5181 - val_loss: 0.8310\n",
      "Epoch 13/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5231 - loss: 0.8273 - val_accuracy: 0.5181 - val_loss: 0.8227\n",
      "Epoch 14/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5295 - loss: 0.8183 - val_accuracy: 0.5181 - val_loss: 0.8148\n",
      "Epoch 15/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5238 - loss: 0.8108 - val_accuracy: 0.5181 - val_loss: 0.8075\n",
      "Epoch 16/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5313 - loss: 0.8039 - val_accuracy: 0.5181 - val_loss: 0.8007\n",
      "Epoch 17/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5059 - loss: 0.7993 - val_accuracy: 0.5233 - val_loss: 0.7944\n",
      "Epoch 18/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5350 - loss: 0.7905 - val_accuracy: 0.5181 - val_loss: 0.7889\n",
      "Epoch 19/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5619 - loss: 0.7804 - val_accuracy: 0.5233 - val_loss: 0.7831\n",
      "Epoch 20/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5185 - loss: 0.7809 - val_accuracy: 0.5233 - val_loss: 0.7783\n",
      "Epoch 21/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5078 - loss: 0.7778 - val_accuracy: 0.5233 - val_loss: 0.7735\n",
      "Epoch 22/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5386 - loss: 0.7696 - val_accuracy: 0.5233 - val_loss: 0.7690\n",
      "Epoch 23/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5083 - loss: 0.7691 - val_accuracy: 0.5233 - val_loss: 0.7649\n",
      "Epoch 24/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5322 - loss: 0.7626 - val_accuracy: 0.5233 - val_loss: 0.7611\n",
      "Epoch 25/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5429 - loss: 0.7571 - val_accuracy: 0.5285 - val_loss: 0.7574\n",
      "Epoch 26/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5413 - loss: 0.7546 - val_accuracy: 0.5285 - val_loss: 0.7544\n",
      "Epoch 27/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5555 - loss: 0.7493 - val_accuracy: 0.5285 - val_loss: 0.7514\n",
      "Epoch 28/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5373 - loss: 0.7485 - val_accuracy: 0.5285 - val_loss: 0.7484\n",
      "Epoch 29/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5411 - loss: 0.7450 - val_accuracy: 0.5285 - val_loss: 0.7458\n",
      "Epoch 30/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5761 - loss: 0.7373 - val_accuracy: 0.5337 - val_loss: 0.7431\n",
      "Epoch 31/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5445 - loss: 0.7404 - val_accuracy: 0.5285 - val_loss: 0.7412\n",
      "Epoch 32/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5093 - loss: 0.7433 - val_accuracy: 0.5285 - val_loss: 0.7390\n",
      "Epoch 33/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5319 - loss: 0.7386 - val_accuracy: 0.5285 - val_loss: 0.7369\n",
      "Epoch 34/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5270 - loss: 0.7381 - val_accuracy: 0.5285 - val_loss: 0.7349\n",
      "Epoch 35/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5398 - loss: 0.7331 - val_accuracy: 0.5285 - val_loss: 0.7335\n",
      "Epoch 36/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5457 - loss: 0.7319 - val_accuracy: 0.5285 - val_loss: 0.7316\n",
      "Epoch 37/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5293 - loss: 0.7302 - val_accuracy: 0.5337 - val_loss: 0.7301\n",
      "Epoch 38/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5201 - loss: 0.7303 - val_accuracy: 0.5285 - val_loss: 0.7292\n",
      "Epoch 39/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5419 - loss: 0.7253 - val_accuracy: 0.5337 - val_loss: 0.7269\n",
      "Epoch 40/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5432 - loss: 0.7257 - val_accuracy: 0.5337 - val_loss: 0.7259\n",
      "Epoch 41/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5534 - loss: 0.7245 - val_accuracy: 0.5492 - val_loss: 0.7242\n",
      "Epoch 42/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5721 - loss: 0.7209 - val_accuracy: 0.5751 - val_loss: 0.7231\n",
      "Epoch 43/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5503 - loss: 0.7239 - val_accuracy: 0.5337 - val_loss: 0.7227\n",
      "Epoch 44/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5316 - loss: 0.7223 - val_accuracy: 0.5389 - val_loss: 0.7214\n",
      "Epoch 45/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5415 - loss: 0.7197 - val_accuracy: 0.5596 - val_loss: 0.7206\n",
      "Epoch 46/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5358 - loss: 0.7180 - val_accuracy: 0.5440 - val_loss: 0.7198\n",
      "Epoch 47/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5509 - loss: 0.7157 - val_accuracy: 0.5648 - val_loss: 0.7183\n",
      "Epoch 48/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5427 - loss: 0.7193 - val_accuracy: 0.5337 - val_loss: 0.7182\n",
      "Epoch 49/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5458 - loss: 0.7149 - val_accuracy: 0.5492 - val_loss: 0.7173\n",
      "Epoch 50/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5659 - loss: 0.7101 - val_accuracy: 0.5699 - val_loss: 0.7158\n",
      "Epoch 51/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5591 - loss: 0.7146 - val_accuracy: 0.5648 - val_loss: 0.7154\n",
      "Epoch 52/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5469 - loss: 0.7152 - val_accuracy: 0.5440 - val_loss: 0.7156\n",
      "Epoch 53/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5498 - loss: 0.7089 - val_accuracy: 0.5699 - val_loss: 0.7142\n",
      "Epoch 54/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5603 - loss: 0.7126 - val_accuracy: 0.5803 - val_loss: 0.7133\n",
      "Epoch 55/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5446 - loss: 0.7138 - val_accuracy: 0.5492 - val_loss: 0.7139\n",
      "Epoch 56/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5489 - loss: 0.7090 - val_accuracy: 0.5648 - val_loss: 0.7121\n",
      "Epoch 57/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5662 - loss: 0.7076 - val_accuracy: 0.5699 - val_loss: 0.7114\n",
      "Epoch 58/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5634 - loss: 0.7070 - val_accuracy: 0.5699 - val_loss: 0.7106\n",
      "Epoch 59/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5850 - loss: 0.7017 - val_accuracy: 0.5648 - val_loss: 0.7094\n",
      "Epoch 60/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5696 - loss: 0.7080 - val_accuracy: 0.5751 - val_loss: 0.7102\n",
      "Epoch 61/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5731 - loss: 0.7076 - val_accuracy: 0.5648 - val_loss: 0.7099\n",
      "Epoch 62/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5750 - loss: 0.7060 - val_accuracy: 0.5440 - val_loss: 0.7086\n",
      "Epoch 63/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5845 - loss: 0.7026 - val_accuracy: 0.5544 - val_loss: 0.7081\n",
      "Epoch 64/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5642 - loss: 0.7072 - val_accuracy: 0.5544 - val_loss: 0.7098\n",
      "Epoch 65/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5609 - loss: 0.7065 - val_accuracy: 0.5648 - val_loss: 0.7086\n",
      "Epoch 66/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5773 - loss: 0.7004 - val_accuracy: 0.5440 - val_loss: 0.7071\n",
      "Epoch 67/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5355 - loss: 0.7113 - val_accuracy: 0.5389 - val_loss: 0.7072\n",
      "Epoch 68/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5821 - loss: 0.7011 - val_accuracy: 0.5596 - val_loss: 0.7065\n",
      "Epoch 69/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5652 - loss: 0.7051 - val_accuracy: 0.5596 - val_loss: 0.7072\n",
      "Epoch 70/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5876 - loss: 0.7021 - val_accuracy: 0.5596 - val_loss: 0.7073\n",
      "Epoch 71/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 0.6938 - val_accuracy: 0.5544 - val_loss: 0.7057\n",
      "Epoch 72/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5967 - loss: 0.7026 - val_accuracy: 0.5440 - val_loss: 0.7065\n",
      "Epoch 73/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5797 - loss: 0.7000 - val_accuracy: 0.5544 - val_loss: 0.7056\n",
      "Epoch 74/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6046 - loss: 0.6956 - val_accuracy: 0.5492 - val_loss: 0.7049\n",
      "Epoch 75/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5702 - loss: 0.6999 - val_accuracy: 0.5648 - val_loss: 0.7052\n",
      "Epoch 76/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5604 - loss: 0.7030 - val_accuracy: 0.5492 - val_loss: 0.7063\n",
      "Epoch 77/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5943 - loss: 0.6948 - val_accuracy: 0.5699 - val_loss: 0.7048\n",
      "Epoch 78/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5874 - loss: 0.6992 - val_accuracy: 0.5492 - val_loss: 0.7049\n",
      "Epoch 79/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6021 - loss: 0.6915 - val_accuracy: 0.5337 - val_loss: 0.7038\n",
      "Epoch 80/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5621 - loss: 0.7020 - val_accuracy: 0.5544 - val_loss: 0.7058\n",
      "Epoch 81/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5838 - loss: 0.6986 - val_accuracy: 0.5648 - val_loss: 0.7047\n",
      "Epoch 82/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5837 - loss: 0.6949 - val_accuracy: 0.5285 - val_loss: 0.7036\n",
      "Epoch 83/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5653 - loss: 0.6958 - val_accuracy: 0.5699 - val_loss: 0.7048\n",
      "Epoch 84/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5899 - loss: 0.6905 - val_accuracy: 0.5492 - val_loss: 0.7037\n",
      "Epoch 85/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6095 - loss: 0.6921 - val_accuracy: 0.5751 - val_loss: 0.7041\n",
      "Epoch 86/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6199 - loss: 0.6959 - val_accuracy: 0.5492 - val_loss: 0.7028\n",
      "Epoch 87/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5658 - loss: 0.7029 - val_accuracy: 0.5751 - val_loss: 0.7046\n",
      "Epoch 88/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5917 - loss: 0.6927 - val_accuracy: 0.5337 - val_loss: 0.7026\n",
      "Epoch 89/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5827 - loss: 0.6936 - val_accuracy: 0.5648 - val_loss: 0.7053\n",
      "Epoch 90/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6082 - loss: 0.6920 - val_accuracy: 0.5337 - val_loss: 0.7017\n",
      "Epoch 91/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5980 - loss: 0.6887 - val_accuracy: 0.5440 - val_loss: 0.7027\n",
      "Epoch 92/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5968 - loss: 0.6924 - val_accuracy: 0.5285 - val_loss: 0.7019\n",
      "Epoch 93/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6001 - loss: 0.6901 - val_accuracy: 0.5699 - val_loss: 0.7019\n",
      "Epoch 94/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5914 - loss: 0.6920 - val_accuracy: 0.5233 - val_loss: 0.7019\n",
      "Epoch 95/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5874 - loss: 0.6903 - val_accuracy: 0.5233 - val_loss: 0.7013\n",
      "Epoch 96/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5652 - loss: 0.6966 - val_accuracy: 0.5285 - val_loss: 0.7012\n",
      "Epoch 97/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6294 - loss: 0.6787 - val_accuracy: 0.5233 - val_loss: 0.7014\n",
      "Epoch 98/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5920 - loss: 0.6886 - val_accuracy: 0.5389 - val_loss: 0.7023\n",
      "Epoch 99/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6004 - loss: 0.6905 - val_accuracy: 0.5337 - val_loss: 0.7016\n",
      "Epoch 100/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5806 - loss: 0.6953 - val_accuracy: 0.5492 - val_loss: 0.7010\n",
      "Restoring model weights from the end of the best epoch: 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6119 - loss: 0.6824\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "테스트 손실: 0.6914379000663757\n",
      "테스트 정확도: 0.5767635107040405\n",
      "테스트 정밀도: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 초기화 함수 선택\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential([\n",
    "    GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.003)),\n",
    "    Dense(32, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.003)),\n",
    "    Dense(16, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.003)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 테스트 데이터로 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'테스트 손실: {loss}')\n",
    "print(f'테스트 정확도: {accuracy}')\n",
    "print(f'테스트 정밀도: {precision}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cbb12",
   "metadata": {},
   "source": [
    "## data 20년치로 늘려 딥러닝 성능 향상 확인\n",
    "- 장점: 데이터셋 증가로 인한 딥러닝 성능 증가\n",
    "- 단점: 중요한 변수의 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f17444",
   "metadata": {},
   "source": [
    "### 데이터 불러오기 20년치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afb4ddd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Change', '5MA', '5Disparity',\n",
       "       '20MA', '20Disparity', '60MA', '60Disparity', '120MA', '120Disparity',\n",
       "       'bol_high', 'bol_low', 'PPO', 'PVO', 'SO', 'Williams_R', 'EOM', 'MFI',\n",
       "       'NVI', 'OBV', 'ATR', 'UI', 'ADX', 'MACD', 'TRIX', 'CCI', 'RMI', 'VHF',\n",
       "       'C_O', 'H_L', 'H_C', 'C_L', 'Cos_sim'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('cos_sk_data.csv')\n",
    "\n",
    "data.index = data[\"Date\"]\n",
    "data.drop(\"Date\", axis = 1, inplace = True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf7a2027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>C_L</th>\n",
       "      <th>Cos_sim</th>\n",
       "      <th>Fluctuation</th>\n",
       "      <th>wil</th>\n",
       "      <th>buy</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>5740</td>\n",
       "      <td>5750</td>\n",
       "      <td>5570</td>\n",
       "      <td>5600</td>\n",
       "      <td>4148379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5338</td>\n",
       "      <td>1.049082</td>\n",
       "      <td>6049.5</td>\n",
       "      <td>0.925696</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1108.479980</td>\n",
       "      <td>2006.680054</td>\n",
       "      <td>424.399994</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>1195.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>5610</td>\n",
       "      <td>6360</td>\n",
       "      <td>5560</td>\n",
       "      <td>6250</td>\n",
       "      <td>18766562</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>5548</td>\n",
       "      <td>1.126532</td>\n",
       "      <td>6024.5</td>\n",
       "      <td>1.037430</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1122.219971</td>\n",
       "      <td>2047.359985</td>\n",
       "      <td>424.399994</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>1178.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>6380</td>\n",
       "      <td>6470</td>\n",
       "      <td>6070</td>\n",
       "      <td>6130</td>\n",
       "      <td>10501167</td>\n",
       "      <td>-0.019200</td>\n",
       "      <td>5766</td>\n",
       "      <td>1.063129</td>\n",
       "      <td>5998.0</td>\n",
       "      <td>1.022007</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6780.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1123.670044</td>\n",
       "      <td>2057.370117</td>\n",
       "      <td>422.799988</td>\n",
       "      <td>33.700001</td>\n",
       "      <td>1189.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>6280</td>\n",
       "      <td>6850</td>\n",
       "      <td>6230</td>\n",
       "      <td>6620</td>\n",
       "      <td>20562472</td>\n",
       "      <td>0.079935</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.096026</td>\n",
       "      <td>6006.5</td>\n",
       "      <td>1.102139</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6480.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1126.329956</td>\n",
       "      <td>2077.679932</td>\n",
       "      <td>421.899994</td>\n",
       "      <td>33.619999</td>\n",
       "      <td>1176.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>6800</td>\n",
       "      <td>6970</td>\n",
       "      <td>6500</td>\n",
       "      <td>6510</td>\n",
       "      <td>13864093</td>\n",
       "      <td>-0.016616</td>\n",
       "      <td>6222</td>\n",
       "      <td>1.046287</td>\n",
       "      <td>6017.5</td>\n",
       "      <td>1.081845</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1131.920044</td>\n",
       "      <td>2100.250000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>1171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-22</th>\n",
       "      <td>154500</td>\n",
       "      <td>156500</td>\n",
       "      <td>152600</td>\n",
       "      <td>156500</td>\n",
       "      <td>5966352</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>150640</td>\n",
       "      <td>1.038901</td>\n",
       "      <td>142040.0</td>\n",
       "      <td>1.101802</td>\n",
       "      <td>...</td>\n",
       "      <td>3900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>157050.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5087.029785</td>\n",
       "      <td>16041.620117</td>\n",
       "      <td>2019.699951</td>\n",
       "      <td>78.610001</td>\n",
       "      <td>1332.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>166900</td>\n",
       "      <td>166900</td>\n",
       "      <td>159800</td>\n",
       "      <td>161400</td>\n",
       "      <td>7012032</td>\n",
       "      <td>0.031310</td>\n",
       "      <td>153560</td>\n",
       "      <td>1.051055</td>\n",
       "      <td>143035.0</td>\n",
       "      <td>1.128395</td>\n",
       "      <td>...</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168850.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5088.799805</td>\n",
       "      <td>15996.820313</td>\n",
       "      <td>2038.599976</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>1327.170044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-26</th>\n",
       "      <td>158400</td>\n",
       "      <td>164800</td>\n",
       "      <td>158400</td>\n",
       "      <td>161800</td>\n",
       "      <td>4329477</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>155660</td>\n",
       "      <td>1.039445</td>\n",
       "      <td>144255.0</td>\n",
       "      <td>1.121625</td>\n",
       "      <td>...</td>\n",
       "      <td>3400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161950.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5069.529785</td>\n",
       "      <td>15976.250000</td>\n",
       "      <td>2028.500000</td>\n",
       "      <td>77.580002</td>\n",
       "      <td>1330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>159000</td>\n",
       "      <td>160000</td>\n",
       "      <td>153300</td>\n",
       "      <td>153800</td>\n",
       "      <td>6107954</td>\n",
       "      <td>-0.049444</td>\n",
       "      <td>156500</td>\n",
       "      <td>0.982748</td>\n",
       "      <td>145145.0</td>\n",
       "      <td>1.059630</td>\n",
       "      <td>...</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5078.180176</td>\n",
       "      <td>16035.299805</td>\n",
       "      <td>2034.000000</td>\n",
       "      <td>78.870003</td>\n",
       "      <td>1331.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-28</th>\n",
       "      <td>154500</td>\n",
       "      <td>159200</td>\n",
       "      <td>153800</td>\n",
       "      <td>158000</td>\n",
       "      <td>3666960</td>\n",
       "      <td>0.027308</td>\n",
       "      <td>158300</td>\n",
       "      <td>0.998105</td>\n",
       "      <td>146295.0</td>\n",
       "      <td>1.080010</td>\n",
       "      <td>...</td>\n",
       "      <td>4200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157850.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5069.759766</td>\n",
       "      <td>15947.740234</td>\n",
       "      <td>2033.000000</td>\n",
       "      <td>78.540001</td>\n",
       "      <td>1331.790039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4985 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close    Volume    Change     5MA   \n",
       "Date                                                                     \n",
       "2004-01-02    5740    5750    5570    5600   4148379  0.000000    5338  \\\n",
       "2004-01-05    5610    6360    5560    6250  18766562  0.116071    5548   \n",
       "2004-01-06    6380    6470    6070    6130  10501167 -0.019200    5766   \n",
       "2004-01-07    6280    6850    6230    6620  20562472  0.079935    6040   \n",
       "2004-01-08    6800    6970    6500    6510  13864093 -0.016616    6222   \n",
       "...            ...     ...     ...     ...       ...       ...     ...   \n",
       "2024-02-22  154500  156500  152600  156500   5966352  0.050336  150640   \n",
       "2024-02-23  166900  166900  159800  161400   7012032  0.031310  153560   \n",
       "2024-02-26  158400  164800  158400  161800   4329477  0.002478  155660   \n",
       "2024-02-27  159000  160000  153300  153800   6107954 -0.049444  156500   \n",
       "2024-02-28  154500  159200  153800  158000   3666960  0.027308  158300   \n",
       "\n",
       "            5Disparity      20MA  20Disparity  ...   C_L  Cos_sim   \n",
       "Date                                           ...                  \n",
       "2004-01-02    1.049082    6049.5     0.925696  ...    30        0  \\\n",
       "2004-01-05    1.126532    6024.5     1.037430  ...   690        0   \n",
       "2004-01-06    1.063129    5998.0     1.022007  ...    60        0   \n",
       "2004-01-07    1.096026    6006.5     1.102139  ...   390        0   \n",
       "2004-01-08    1.046287    6017.5     1.081845  ...    10        0   \n",
       "...                ...       ...          ...  ...   ...      ...   \n",
       "2024-02-22    1.038901  142040.0     1.101802  ...  3900        0   \n",
       "2024-02-23    1.051055  143035.0     1.128395  ...  1600        0   \n",
       "2024-02-26    1.039445  144255.0     1.121625  ...  3400        0   \n",
       "2024-02-27    0.982748  145145.0     1.059630  ...   500        0   \n",
       "2024-02-28    0.998105  146295.0     1.080010  ...  4200        1   \n",
       "\n",
       "            Fluctuation       wil  buy      S&P 500        NASDAQ   \n",
       "Date                                                                \n",
       "2004-01-02            0    5700.0    0  1108.479980   2006.680054  \\\n",
       "2004-01-05            1    5700.0    1  1122.219971   2047.359985   \n",
       "2004-01-06            0    6780.0    0  1123.670044   2057.370117   \n",
       "2004-01-07            1    6480.0    1  1126.329956   2077.679932   \n",
       "2004-01-08            0    7110.0    0  1131.920044   2100.250000   \n",
       "...                 ...       ...  ...          ...           ...   \n",
       "2024-02-22            1  157050.0    0  5087.029785  16041.620117   \n",
       "2024-02-23            1  168850.0    0  5088.799805  15996.820313   \n",
       "2024-02-26            1  161950.0    1  5069.529785  15976.250000   \n",
       "2024-02-27            0  162200.0    0  5078.180176  16035.299805   \n",
       "2024-02-28            1  157850.0    1  5069.759766  15947.740234   \n",
       "\n",
       "                   Gold        Oil      USD/KRW  \n",
       "Date                                             \n",
       "2004-01-02   424.399994  33.779999  1195.800049  \n",
       "2004-01-05   424.399994  33.779999  1178.900024  \n",
       "2004-01-06   422.799988  33.700001  1189.400024  \n",
       "2004-01-07   421.899994  33.619999  1176.800049  \n",
       "2004-01-08   424.000000  33.980000  1171.000000  \n",
       "...                 ...        ...          ...  \n",
       "2024-02-22  2019.699951  78.610001  1332.869995  \n",
       "2024-02-23  2038.599976  76.489998  1327.170044  \n",
       "2024-02-26  2028.500000  77.580002  1330.000000  \n",
       "2024-02-27  2034.000000  78.870003  1331.719971  \n",
       "2024-02-28  2033.000000  78.540001  1331.790039  \n",
       "\n",
       "[4985 rows x 45 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3583ac14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fluctuation\n",
       "0    2575\n",
       "1    2450\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Fluctuation\"] = 0\n",
    "data.loc[data[\"Change\"] > 0, \"Fluctuation\"] = 1\n",
    "data[\"Fluctuation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "756e56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wil 지표 산출\n",
    "for i in range(1, len(data)):\n",
    "    data.loc[data.index[i], 'wil'] = data.iloc[i]['Open'] + (data.iloc[i-1]['High'] - data.iloc[i-1]['Low']) *0.5\n",
    "\n",
    "# 고가가 wil 보다 높을 때 1 아닐 때  0 분류 \n",
    "data['buy'] = (data['wil'] < data['High']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b51de7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Close         Close        Close      Close        Close\n",
      "Date                                                                      \n",
      "2004-01-02  1108.479980   2006.680054          NaN        NaN  1195.800049\n",
      "2004-01-05  1122.219971   2047.359985   424.399994  33.779999  1178.900024\n",
      "2004-01-06  1123.670044   2057.370117   422.799988  33.700001  1189.400024\n",
      "2004-01-07  1126.329956   2077.679932   421.899994  33.619999  1176.800049\n",
      "2004-01-08  1131.920044   2100.250000   424.000000  33.980000  1171.000000\n",
      "...                 ...           ...          ...        ...          ...\n",
      "2024-02-22  5087.029785  16041.620117  2019.699951  78.610001  1332.869995\n",
      "2024-02-23  5088.799805  15996.820313  2038.599976  76.489998  1327.170044\n",
      "2024-02-26  5069.529785  15976.250000  2028.500000  77.580002  1330.000000\n",
      "2024-02-27  5078.180176  16035.299805  2034.000000  78.870003  1331.719971\n",
      "2024-02-28  5069.759766  15947.740234  2033.000000  78.540001  1331.790039\n",
      "\n",
      "[5259 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>C_L</th>\n",
       "      <th>Cos_sim</th>\n",
       "      <th>Fluctuation</th>\n",
       "      <th>wil</th>\n",
       "      <th>buy</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>5740</td>\n",
       "      <td>5750</td>\n",
       "      <td>5570</td>\n",
       "      <td>5600</td>\n",
       "      <td>4148379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5338</td>\n",
       "      <td>1.049082</td>\n",
       "      <td>6049.5</td>\n",
       "      <td>0.925696</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1108.479980</td>\n",
       "      <td>2006.680054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1195.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>5610</td>\n",
       "      <td>6360</td>\n",
       "      <td>5560</td>\n",
       "      <td>6250</td>\n",
       "      <td>18766562</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>5548</td>\n",
       "      <td>1.126532</td>\n",
       "      <td>6024.5</td>\n",
       "      <td>1.037430</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1122.219971</td>\n",
       "      <td>2047.359985</td>\n",
       "      <td>424.399994</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>1178.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>6380</td>\n",
       "      <td>6470</td>\n",
       "      <td>6070</td>\n",
       "      <td>6130</td>\n",
       "      <td>10501167</td>\n",
       "      <td>-0.019200</td>\n",
       "      <td>5766</td>\n",
       "      <td>1.063129</td>\n",
       "      <td>5998.0</td>\n",
       "      <td>1.022007</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6780.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1123.670044</td>\n",
       "      <td>2057.370117</td>\n",
       "      <td>422.799988</td>\n",
       "      <td>33.700001</td>\n",
       "      <td>1189.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>6280</td>\n",
       "      <td>6850</td>\n",
       "      <td>6230</td>\n",
       "      <td>6620</td>\n",
       "      <td>20562472</td>\n",
       "      <td>0.079935</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.096026</td>\n",
       "      <td>6006.5</td>\n",
       "      <td>1.102139</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6480.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1126.329956</td>\n",
       "      <td>2077.679932</td>\n",
       "      <td>421.899994</td>\n",
       "      <td>33.619999</td>\n",
       "      <td>1176.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>6800</td>\n",
       "      <td>6970</td>\n",
       "      <td>6500</td>\n",
       "      <td>6510</td>\n",
       "      <td>13864093</td>\n",
       "      <td>-0.016616</td>\n",
       "      <td>6222</td>\n",
       "      <td>1.046287</td>\n",
       "      <td>6017.5</td>\n",
       "      <td>1.081845</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1131.920044</td>\n",
       "      <td>2100.250000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>1171.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open  High   Low  Close    Volume    Change   5MA  5Disparity   \n",
       "Date                                                                        \n",
       "2004-01-02  5740  5750  5570   5600   4148379  0.000000  5338    1.049082  \\\n",
       "2004-01-05  5610  6360  5560   6250  18766562  0.116071  5548    1.126532   \n",
       "2004-01-06  6380  6470  6070   6130  10501167 -0.019200  5766    1.063129   \n",
       "2004-01-07  6280  6850  6230   6620  20562472  0.079935  6040    1.096026   \n",
       "2004-01-08  6800  6970  6500   6510  13864093 -0.016616  6222    1.046287   \n",
       "\n",
       "              20MA  20Disparity  ...  C_L  Cos_sim  Fluctuation     wil  buy   \n",
       "Date                             ...                                           \n",
       "2004-01-02  6049.5     0.925696  ...   30        0            0     NaN    0  \\\n",
       "2004-01-05  6024.5     1.037430  ...  690        0            1  5700.0    1   \n",
       "2004-01-06  5998.0     1.022007  ...   60        0            0  6780.0    0   \n",
       "2004-01-07  6006.5     1.102139  ...  390        0            1  6480.0    1   \n",
       "2004-01-08  6017.5     1.081845  ...   10        0            0  7110.0    0   \n",
       "\n",
       "                S&P 500       NASDAQ        Gold        Oil      USD/KRW  \n",
       "Date                                                                      \n",
       "2004-01-02  1108.479980  2006.680054         NaN        NaN  1195.800049  \n",
       "2004-01-05  1122.219971  2047.359985  424.399994  33.779999  1178.900024  \n",
       "2004-01-06  1123.670044  2057.370117  422.799988  33.700001  1189.400024  \n",
       "2004-01-07  1126.329956  2077.679932  421.899994  33.619999  1176.800049  \n",
       "2004-01-08  1131.920044  2100.250000  424.000000  33.980000  1171.000000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Open              0\n",
       "High              0\n",
       "Low               0\n",
       "Close             0\n",
       "Volume            0\n",
       "Change            0\n",
       "5MA               0\n",
       "5Disparity        0\n",
       "20MA              0\n",
       "20Disparity       0\n",
       "60MA              0\n",
       "60Disparity       0\n",
       "120MA             0\n",
       "120Disparity      0\n",
       "bol_high          0\n",
       "bol_low           0\n",
       "PPO               0\n",
       "PVO               0\n",
       "SO                0\n",
       "Williams_R        0\n",
       "EOM               0\n",
       "MFI               0\n",
       "NVI               0\n",
       "OBV               0\n",
       "ATR               0\n",
       "UI                0\n",
       "ADX               0\n",
       "MACD              0\n",
       "TRIX              0\n",
       "CCI               0\n",
       "RMI               0\n",
       "VHF               0\n",
       "C_O               0\n",
       "H_L               0\n",
       "H_C               0\n",
       "C_L               0\n",
       "Cos_sim           0\n",
       "Fluctuation       0\n",
       "wil               1\n",
       "buy               0\n",
       "S&P 500         155\n",
       "NASDAQ          155\n",
       "Gold            163\n",
       "Oil             159\n",
       "USD/KRW          29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Open            0\n",
       "High            0\n",
       "Low             0\n",
       "Close           0\n",
       "Volume          0\n",
       "Change          0\n",
       "5MA             0\n",
       "5Disparity      0\n",
       "20MA            0\n",
       "20Disparity     0\n",
       "60MA            0\n",
       "60Disparity     0\n",
       "120MA           0\n",
       "120Disparity    0\n",
       "bol_high        0\n",
       "bol_low         0\n",
       "PPO             0\n",
       "PVO             0\n",
       "SO              0\n",
       "Williams_R      0\n",
       "EOM             0\n",
       "MFI             0\n",
       "NVI             0\n",
       "OBV             0\n",
       "ATR             0\n",
       "UI              0\n",
       "ADX             0\n",
       "MACD            0\n",
       "TRIX            0\n",
       "CCI             0\n",
       "RMI             0\n",
       "VHF             0\n",
       "C_O             0\n",
       "H_L             0\n",
       "H_C             0\n",
       "C_L             0\n",
       "Cos_sim         0\n",
       "Fluctuation     0\n",
       "wil             0\n",
       "buy             0\n",
       "S&P 500         0\n",
       "NASDAQ          0\n",
       "Gold            0\n",
       "Oil             0\n",
       "USD/KRW         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>5MA</th>\n",
       "      <th>5Disparity</th>\n",
       "      <th>20MA</th>\n",
       "      <th>20Disparity</th>\n",
       "      <th>...</th>\n",
       "      <th>C_L</th>\n",
       "      <th>Cos_sim</th>\n",
       "      <th>Fluctuation</th>\n",
       "      <th>wil</th>\n",
       "      <th>buy</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oil</th>\n",
       "      <th>USD/KRW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>5740</td>\n",
       "      <td>5750</td>\n",
       "      <td>5570</td>\n",
       "      <td>5600</td>\n",
       "      <td>4148379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5338</td>\n",
       "      <td>1.049082</td>\n",
       "      <td>6049.5</td>\n",
       "      <td>0.925696</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1108.479980</td>\n",
       "      <td>2006.680054</td>\n",
       "      <td>424.399994</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>1195.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>5610</td>\n",
       "      <td>6360</td>\n",
       "      <td>5560</td>\n",
       "      <td>6250</td>\n",
       "      <td>18766562</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>5548</td>\n",
       "      <td>1.126532</td>\n",
       "      <td>6024.5</td>\n",
       "      <td>1.037430</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1122.219971</td>\n",
       "      <td>2047.359985</td>\n",
       "      <td>424.399994</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>1178.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>6380</td>\n",
       "      <td>6470</td>\n",
       "      <td>6070</td>\n",
       "      <td>6130</td>\n",
       "      <td>10501167</td>\n",
       "      <td>-0.019200</td>\n",
       "      <td>5766</td>\n",
       "      <td>1.063129</td>\n",
       "      <td>5998.0</td>\n",
       "      <td>1.022007</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6780.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1123.670044</td>\n",
       "      <td>2057.370117</td>\n",
       "      <td>422.799988</td>\n",
       "      <td>33.700001</td>\n",
       "      <td>1189.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>6280</td>\n",
       "      <td>6850</td>\n",
       "      <td>6230</td>\n",
       "      <td>6620</td>\n",
       "      <td>20562472</td>\n",
       "      <td>0.079935</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.096026</td>\n",
       "      <td>6006.5</td>\n",
       "      <td>1.102139</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6480.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1126.329956</td>\n",
       "      <td>2077.679932</td>\n",
       "      <td>421.899994</td>\n",
       "      <td>33.619999</td>\n",
       "      <td>1176.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>6800</td>\n",
       "      <td>6970</td>\n",
       "      <td>6500</td>\n",
       "      <td>6510</td>\n",
       "      <td>13864093</td>\n",
       "      <td>-0.016616</td>\n",
       "      <td>6222</td>\n",
       "      <td>1.046287</td>\n",
       "      <td>6017.5</td>\n",
       "      <td>1.081845</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1131.920044</td>\n",
       "      <td>2100.250000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>1171.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open  High   Low  Close    Volume    Change   5MA  5Disparity   \n",
       "Date                                                                        \n",
       "2004-01-02  5740  5750  5570   5600   4148379  0.000000  5338    1.049082  \\\n",
       "2004-01-05  5610  6360  5560   6250  18766562  0.116071  5548    1.126532   \n",
       "2004-01-06  6380  6470  6070   6130  10501167 -0.019200  5766    1.063129   \n",
       "2004-01-07  6280  6850  6230   6620  20562472  0.079935  6040    1.096026   \n",
       "2004-01-08  6800  6970  6500   6510  13864093 -0.016616  6222    1.046287   \n",
       "\n",
       "              20MA  20Disparity  ...  C_L  Cos_sim  Fluctuation     wil  buy   \n",
       "Date                             ...                                           \n",
       "2004-01-02  6049.5     0.925696  ...   30        0            0  5700.0    0  \\\n",
       "2004-01-05  6024.5     1.037430  ...  690        0            1  5700.0    1   \n",
       "2004-01-06  5998.0     1.022007  ...   60        0            0  6780.0    0   \n",
       "2004-01-07  6006.5     1.102139  ...  390        0            1  6480.0    1   \n",
       "2004-01-08  6017.5     1.081845  ...   10        0            0  7110.0    0   \n",
       "\n",
       "                S&P 500       NASDAQ        Gold        Oil      USD/KRW  \n",
       "Date                                                                      \n",
       "2004-01-02  1108.479980  2006.680054  424.399994  33.779999  1195.800049  \n",
       "2004-01-05  1122.219971  2047.359985  424.399994  33.779999  1178.900024  \n",
       "2004-01-06  1123.670044  2057.370117  422.799988  33.700001  1189.400024  \n",
       "2004-01-07  1126.329956  2077.679932  421.899994  33.619999  1176.800049  \n",
       "2004-01-08  1131.920044  2100.250000  424.000000  33.980000  1171.000000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "\n",
    "# 데이터 가져오기\n",
    "sp500 = fdr.DataReader('^GSPC', '2004-01-02', '2024-02-29')\n",
    "nasdaq = fdr.DataReader('^IXIC', '2004-01-02', '2024-02-29')\n",
    "gold = fdr.DataReader('GC=F', '2004-01-02', '2024-02-29')\n",
    "oil = fdr.DataReader('CL=F', '2004-01-02', '2024-02-29')\n",
    "usdkrw = fdr.DataReader('USD/KRW', '2004-01-02', '2024-02-29')\n",
    "\n",
    "# 데이터프레임 병합\n",
    "data_eco = pd.concat([sp500['Close'], nasdaq['Close'], gold['Close'], oil['Close'], usdkrw['Close']], axis=1)\n",
    "\n",
    "# 중복된 인덱스 제거\n",
    "data_eco = data_eco.loc[~data_eco.index.duplicated()]\n",
    "\n",
    "print(data_eco)\n",
    "\n",
    "data_eco.columns = ['S&P 500', 'NASDAQ', 'Gold', 'Oil', 'USD/KRW']\n",
    "\n",
    "isnadata=data_eco[data_eco.isnull().any(axis=1)]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "merged_data = pd.merge(data, data_eco, left_index=True, right_index=True)\n",
    "merged_data.head()\n",
    "merged_data.isna().sum()\n",
    "# 결측치를 'bfill'로 처리하기\n",
    "data_df_filled = merged_data.fillna(method='bfill')\n",
    "data_df_filled.isnull().sum()\n",
    "data_df_filled.head()\n",
    "data=data_df_filled.copy()\n",
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98bdb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Fluctuation\"\n",
    "SEQ_SIZE = 30        # 30 / 60 / 120\n",
    "PRED_SIZE = 1\n",
    "BATCH_SIZE = 1                        # 1 / 4 / 8\n",
    "HIDDEN_SIZE = 32                      # 64 / 128\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e7a53af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5740.       5750.       5570.       ...  424.399994   33.779999\n",
      "  1195.800049]\n",
      " [5610.       6360.       5560.       ...  424.399994   33.779999\n",
      "  1178.900024]\n",
      " [6380.       6470.       6070.       ...  422.799988   33.700001\n",
      "  1189.400024]\n",
      " ...\n",
      " [8260.       8530.       8250.       ...  410.299988   34.560001\n",
      "  1159.699951]\n",
      " [8360.       8600.       8240.       ...  416.         35.16\n",
      "  1147.699951]\n",
      " [8470.       8850.       8460.       ...  416.         35.16\n",
      "  1156.900024]] \n",
      " [1]\n",
      "X size :  (4955, 30, 45)\n",
      "y size :  (4955, 1)\n"
     ]
    }
   ],
   "source": [
    "def split_xy(dataset, time_steps, y_column):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i + time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "\n",
    "        if y_end_number > len(dataset):\n",
    "            break\n",
    "        tmp_x = dataset.iloc[i:x_end_number, :]  \n",
    "        tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "        x.append(tmp_x.values)  \n",
    "        y.append(tmp_y.values)  \n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "X, y = split_xy(data, SEQ_SIZE, PRED_SIZE)\n",
    "print(X[0],\"\\n\", y[0])\n",
    "print(\"X size : \", X.shape)\n",
    "print(\"y size : \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dfff379a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minye\\AppData\\Local\\Temp\\ipykernel_15844\\496664998.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  scaled_sample = (sample - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "## 표준졍규화-> 시퀀스가 작을 때 표준편차가 0이 되는 경우가 있어서 이를 확인하고 선택해야 함 \n",
    "import numpy as np\n",
    "# 시퀀스별로 정규화\n",
    "def scale_sequences(data):\n",
    "    scaled_data = []\n",
    "    for sample in data:\n",
    "        # 시퀀스별로 평균과 표준 편차 계산\n",
    "        mean = np.mean(sample, axis=0)\n",
    "        std = np.std(sample, axis=0)\n",
    "        if 0 in std:\n",
    "            print('error')\n",
    "        # 표준화 수행\n",
    "        scaled_sample = (sample - mean) / std\n",
    "        scaled_data.append(scaled_sample)\n",
    "    return np.array(scaled_data)\n",
    "\n",
    "# 주식 데이터를 시퀀스별로 표준화\n",
    "X = scale_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a7852cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4955, 30, 45)\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스별로 Min-Max 정규화\n",
    "for i in range(X.shape[0]):  \n",
    "    for j in range(X.shape[2]): \n",
    "        # 시퀀스 내에서 각 피쳐의 최소값과 최대값 계산\n",
    "        min_val = np.min(X[i, :, j])\n",
    "        max_val = np.max(X[i, :, j])\n",
    "        if min_val == max_val:\n",
    "            X[i, :, j] = 0  \n",
    "        else:\n",
    "            X[i, :, j] = (X[i, :, j] - min_val) / (max_val - min_val)\n",
    "\n",
    "# 결과 확인\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab3fb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 3차원 배열 데이터를 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdcb642",
   "metadata": {},
   "source": [
    "### 시퀀스별 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c49d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 시간 간격: 5일, 스케일링 방법: standard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5110 - loss: 0.6937 - val_accuracy: 0.4670 - val_loss: 0.6985\n",
      "Epoch 2/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5469 - loss: 0.6906 - val_accuracy: 0.4772 - val_loss: 0.6989\n",
      "Epoch 3/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5546 - loss: 0.6883 - val_accuracy: 0.4924 - val_loss: 0.6995\n",
      "Epoch 4/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6255 - loss: 0.6812 - val_accuracy: 0.4822 - val_loss: 0.7003\n",
      "Epoch 5/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5933 - loss: 0.6825 - val_accuracy: 0.4721 - val_loss: 0.7009\n",
      "Epoch 6/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5863 - loss: 0.6817 - val_accuracy: 0.4772 - val_loss: 0.7019\n",
      "Epoch 7/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6077 - loss: 0.6777 - val_accuracy: 0.4721 - val_loss: 0.7025\n",
      "Epoch 8/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6188 - loss: 0.6755 - val_accuracy: 0.4670 - val_loss: 0.7033\n",
      "Epoch 9/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6125 - loss: 0.6694 - val_accuracy: 0.4721 - val_loss: 0.7037\n",
      "Epoch 10/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6228 - loss: 0.6729 - val_accuracy: 0.4721 - val_loss: 0.7039\n",
      "Epoch 11/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6272 - loss: 0.6647 - val_accuracy: 0.4772 - val_loss: 0.7048\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5313 - loss: 0.6901 \n",
      "----------------------------\n",
      "현재 시간 간격: 5일, 스케일링 방법: minmax\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4365 - loss: 0.6961 - val_accuracy: 0.4822 - val_loss: 0.6928\n",
      "Epoch 2/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5459 - loss: 0.6920 - val_accuracy: 0.4975 - val_loss: 0.6933\n",
      "Epoch 3/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5098 - loss: 0.6921 - val_accuracy: 0.4873 - val_loss: 0.6938\n",
      "Epoch 4/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5391 - loss: 0.6901 - val_accuracy: 0.4873 - val_loss: 0.6941\n",
      "Epoch 5/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5546 - loss: 0.6892 - val_accuracy: 0.4873 - val_loss: 0.6944\n",
      "Epoch 6/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5067 - loss: 0.6919 - val_accuracy: 0.4873 - val_loss: 0.6947\n",
      "Epoch 7/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5046 - loss: 0.6924 - val_accuracy: 0.4822 - val_loss: 0.6952\n",
      "Epoch 8/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5200 - loss: 0.6909 - val_accuracy: 0.4822 - val_loss: 0.6953\n",
      "Epoch 9/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5219 - loss: 0.6898 - val_accuracy: 0.4822 - val_loss: 0.6952\n",
      "Epoch 10/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5178 - loss: 0.6906 - val_accuracy: 0.4822 - val_loss: 0.6953\n",
      "Epoch 11/100\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5161 - loss: 0.6916 - val_accuracy: 0.4822 - val_loss: 0.6951\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5814 - loss: 0.6908 \n",
      "----------------------------\n",
      "현재 시간 간격: 10일, 스케일링 방법: standard\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4874 - loss: 0.6926 - val_accuracy: 0.5102 - val_loss: 0.6907\n",
      "Epoch 2/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5375 - loss: 0.6899 - val_accuracy: 0.5306 - val_loss: 0.6898\n",
      "Epoch 3/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5368 - loss: 0.6899 - val_accuracy: 0.5408 - val_loss: 0.6891\n",
      "Epoch 4/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5528 - loss: 0.6879 - val_accuracy: 0.5306 - val_loss: 0.6884\n",
      "Epoch 5/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5282 - loss: 0.6890 - val_accuracy: 0.5255 - val_loss: 0.6878\n",
      "Epoch 6/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5668 - loss: 0.6821 - val_accuracy: 0.5204 - val_loss: 0.6870\n",
      "Epoch 7/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5692 - loss: 0.6835 - val_accuracy: 0.5357 - val_loss: 0.6864\n",
      "Epoch 8/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6055 - loss: 0.6790 - val_accuracy: 0.5561 - val_loss: 0.6854\n",
      "Epoch 9/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5982 - loss: 0.6792 - val_accuracy: 0.5510 - val_loss: 0.6851\n",
      "Epoch 10/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5796 - loss: 0.6782 - val_accuracy: 0.5663 - val_loss: 0.6850\n",
      "Epoch 11/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5991 - loss: 0.6736 - val_accuracy: 0.5765 - val_loss: 0.6847\n",
      "Epoch 12/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6118 - loss: 0.6697 - val_accuracy: 0.5816 - val_loss: 0.6847\n",
      "Epoch 13/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5982 - loss: 0.6733 - val_accuracy: 0.5816 - val_loss: 0.6852\n",
      "Epoch 14/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5635 - loss: 0.6749 - val_accuracy: 0.5714 - val_loss: 0.6856\n",
      "Epoch 15/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5851 - loss: 0.6697 - val_accuracy: 0.5867 - val_loss: 0.6853\n",
      "Epoch 16/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5952 - loss: 0.6661 - val_accuracy: 0.5867 - val_loss: 0.6849\n",
      "Epoch 17/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6025 - loss: 0.6697 - val_accuracy: 0.5816 - val_loss: 0.6857\n",
      "Epoch 18/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5981 - loss: 0.6654 - val_accuracy: 0.5816 - val_loss: 0.6857\n",
      "Epoch 19/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6286 - loss: 0.6563 - val_accuracy: 0.5816 - val_loss: 0.6865\n",
      "Epoch 20/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6104 - loss: 0.6633 - val_accuracy: 0.5816 - val_loss: 0.6870\n",
      "Epoch 21/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6230 - loss: 0.6586 - val_accuracy: 0.5867 - val_loss: 0.6875\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5618 - loss: 0.6869 \n",
      "----------------------------\n",
      "현재 시간 간격: 10일, 스케일링 방법: minmax\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4884 - loss: 0.6937 - val_accuracy: 0.5612 - val_loss: 0.6871\n",
      "Epoch 2/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5473 - loss: 0.6869 - val_accuracy: 0.5255 - val_loss: 0.6890\n",
      "Epoch 3/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4779 - loss: 0.6925 - val_accuracy: 0.5153 - val_loss: 0.6882\n",
      "Epoch 4/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5552 - loss: 0.6902 - val_accuracy: 0.5051 - val_loss: 0.6878\n",
      "Epoch 5/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5097 - loss: 0.6911 - val_accuracy: 0.5102 - val_loss: 0.6896\n",
      "Epoch 6/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4998 - loss: 0.6915 - val_accuracy: 0.5153 - val_loss: 0.6887\n",
      "Epoch 7/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5238 - loss: 0.6889 - val_accuracy: 0.5051 - val_loss: 0.6894\n",
      "Epoch 8/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5308 - loss: 0.6906 - val_accuracy: 0.4949 - val_loss: 0.6882\n",
      "Epoch 9/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5134 - loss: 0.6927 - val_accuracy: 0.5051 - val_loss: 0.6886\n",
      "Epoch 10/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5078 - loss: 0.6908 - val_accuracy: 0.5255 - val_loss: 0.6872\n",
      "Epoch 11/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5339 - loss: 0.6873 - val_accuracy: 0.5102 - val_loss: 0.6896\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5855 - loss: 0.6871 \n",
      "----------------------------\n",
      "현재 시간 간격: 15일, 스케일링 방법: standard\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5208 - loss: 0.6927 - val_accuracy: 0.5436 - val_loss: 0.6836\n",
      "Epoch 2/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5308 - loss: 0.6878 - val_accuracy: 0.5487 - val_loss: 0.6832\n",
      "Epoch 3/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5910 - loss: 0.6818 - val_accuracy: 0.5436 - val_loss: 0.6830\n",
      "Epoch 4/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5776 - loss: 0.6833 - val_accuracy: 0.5333 - val_loss: 0.6835\n",
      "Epoch 5/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5936 - loss: 0.6778 - val_accuracy: 0.5436 - val_loss: 0.6831\n",
      "Epoch 6/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5901 - loss: 0.6760 - val_accuracy: 0.5333 - val_loss: 0.6830\n",
      "Epoch 7/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6330 - loss: 0.6699 - val_accuracy: 0.5385 - val_loss: 0.6834\n",
      "Epoch 8/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5905 - loss: 0.6765 - val_accuracy: 0.5385 - val_loss: 0.6833\n",
      "Epoch 9/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6151 - loss: 0.6714 - val_accuracy: 0.5436 - val_loss: 0.6831\n",
      "Epoch 10/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5683 - loss: 0.6783 - val_accuracy: 0.5436 - val_loss: 0.6829\n",
      "Epoch 11/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6188 - loss: 0.6660 - val_accuracy: 0.5385 - val_loss: 0.6834\n",
      "Epoch 12/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6196 - loss: 0.6649 - val_accuracy: 0.5487 - val_loss: 0.6831\n",
      "Epoch 13/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6278 - loss: 0.6581 - val_accuracy: 0.5436 - val_loss: 0.6829\n",
      "Epoch 14/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6101 - loss: 0.6625 - val_accuracy: 0.5590 - val_loss: 0.6823\n",
      "Epoch 15/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6203 - loss: 0.6619 - val_accuracy: 0.5487 - val_loss: 0.6822\n",
      "Epoch 16/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6123 - loss: 0.6572 - val_accuracy: 0.5538 - val_loss: 0.6820\n",
      "Epoch 17/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6370 - loss: 0.6554 - val_accuracy: 0.5436 - val_loss: 0.6823\n",
      "Epoch 18/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6372 - loss: 0.6566 - val_accuracy: 0.5487 - val_loss: 0.6820\n",
      "Epoch 19/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.6451 - loss: 0.6486 - val_accuracy: 0.5436 - val_loss: 0.6822\n",
      "Epoch 20/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6515 - loss: 0.6452 - val_accuracy: 0.5436 - val_loss: 0.6825\n",
      "Epoch 21/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6294 - loss: 0.6510 - val_accuracy: 0.5538 - val_loss: 0.6825\n",
      "Epoch 22/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6209 - loss: 0.6508 - val_accuracy: 0.5590 - val_loss: 0.6818\n",
      "Epoch 23/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6447 - loss: 0.6459 - val_accuracy: 0.5538 - val_loss: 0.6823\n",
      "Epoch 24/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6745 - loss: 0.6399 - val_accuracy: 0.5590 - val_loss: 0.6823\n",
      "Epoch 25/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6568 - loss: 0.6425 - val_accuracy: 0.5590 - val_loss: 0.6828\n",
      "Epoch 26/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6648 - loss: 0.6393 - val_accuracy: 0.5641 - val_loss: 0.6825\n",
      "Epoch 27/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6399 - loss: 0.6344 - val_accuracy: 0.5692 - val_loss: 0.6824\n",
      "Epoch 28/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6641 - loss: 0.6330 - val_accuracy: 0.5949 - val_loss: 0.6825\n",
      "Epoch 29/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6656 - loss: 0.6345 - val_accuracy: 0.5897 - val_loss: 0.6832\n",
      "Epoch 30/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6657 - loss: 0.6247 - val_accuracy: 0.5795 - val_loss: 0.6840\n",
      "Epoch 31/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6956 - loss: 0.6098 - val_accuracy: 0.5897 - val_loss: 0.6842\n",
      "Epoch 32/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6948 - loss: 0.6201 - val_accuracy: 0.5949 - val_loss: 0.6837\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5326 - loss: 0.7017 \n",
      "----------------------------\n",
      "현재 시간 간격: 15일, 스케일링 방법: minmax\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5034 - loss: 0.6956 - val_accuracy: 0.5333 - val_loss: 0.6924\n",
      "Epoch 2/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5024 - loss: 0.6935 - val_accuracy: 0.5231 - val_loss: 0.6910\n",
      "Epoch 3/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5372 - loss: 0.6918 - val_accuracy: 0.5282 - val_loss: 0.6908\n",
      "Epoch 4/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5387 - loss: 0.6891 - val_accuracy: 0.5231 - val_loss: 0.6908\n",
      "Epoch 5/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5549 - loss: 0.6905 - val_accuracy: 0.5487 - val_loss: 0.6906\n",
      "Epoch 6/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5197 - loss: 0.6902 - val_accuracy: 0.5231 - val_loss: 0.6907\n",
      "Epoch 7/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5583 - loss: 0.6891 - val_accuracy: 0.5282 - val_loss: 0.6907\n",
      "Epoch 8/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5470 - loss: 0.6898 - val_accuracy: 0.5436 - val_loss: 0.6907\n",
      "Epoch 9/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5533 - loss: 0.6884 - val_accuracy: 0.4974 - val_loss: 0.6909\n",
      "Epoch 10/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5552 - loss: 0.6860 - val_accuracy: 0.5026 - val_loss: 0.6906\n",
      "Epoch 11/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5853 - loss: 0.6833 - val_accuracy: 0.5282 - val_loss: 0.6908\n",
      "Epoch 12/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5600 - loss: 0.6852 - val_accuracy: 0.4974 - val_loss: 0.6905\n",
      "Epoch 13/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5788 - loss: 0.6857 - val_accuracy: 0.5179 - val_loss: 0.6907\n",
      "Epoch 14/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5421 - loss: 0.6876 - val_accuracy: 0.5128 - val_loss: 0.6906\n",
      "Epoch 15/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5729 - loss: 0.6849 - val_accuracy: 0.5128 - val_loss: 0.6906\n",
      "Epoch 16/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5708 - loss: 0.6833 - val_accuracy: 0.5077 - val_loss: 0.6907\n",
      "Epoch 17/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5544 - loss: 0.6854 - val_accuracy: 0.5179 - val_loss: 0.6911\n",
      "Epoch 18/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5938 - loss: 0.6794 - val_accuracy: 0.5128 - val_loss: 0.6921\n",
      "Epoch 19/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5528 - loss: 0.6883 - val_accuracy: 0.5077 - val_loss: 0.6912\n",
      "Epoch 20/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5710 - loss: 0.6800 - val_accuracy: 0.4974 - val_loss: 0.6918\n",
      "Epoch 21/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5710 - loss: 0.6827 - val_accuracy: 0.5128 - val_loss: 0.6913\n",
      "Epoch 22/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5315 - loss: 0.6879 - val_accuracy: 0.5128 - val_loss: 0.6916\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5197 - loss: 0.6926 \n",
      "----------------------------\n",
      "현재 시간 간격: 30일, 스케일링 방법: standard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5216 - loss: 0.6927 - val_accuracy: 0.5440 - val_loss: 0.6941\n",
      "Epoch 2/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5399 - loss: 0.6894 - val_accuracy: 0.5337 - val_loss: 0.6938\n",
      "Epoch 3/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5615 - loss: 0.6856 - val_accuracy: 0.5440 - val_loss: 0.6936\n",
      "Epoch 4/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5236 - loss: 0.6894 - val_accuracy: 0.5440 - val_loss: 0.6926\n",
      "Epoch 5/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5519 - loss: 0.6841 - val_accuracy: 0.5544 - val_loss: 0.6923\n",
      "Epoch 6/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5528 - loss: 0.6836 - val_accuracy: 0.5285 - val_loss: 0.6925\n",
      "Epoch 7/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5565 - loss: 0.6888 - val_accuracy: 0.5285 - val_loss: 0.6920\n",
      "Epoch 8/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5496 - loss: 0.6813 - val_accuracy: 0.5181 - val_loss: 0.6920\n",
      "Epoch 9/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5670 - loss: 0.6811 - val_accuracy: 0.5078 - val_loss: 0.6918\n",
      "Epoch 10/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5895 - loss: 0.6766 - val_accuracy: 0.5130 - val_loss: 0.6911\n",
      "Epoch 11/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5584 - loss: 0.6806 - val_accuracy: 0.5233 - val_loss: 0.6911\n",
      "Epoch 12/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5488 - loss: 0.6846 - val_accuracy: 0.5233 - val_loss: 0.6914\n",
      "Epoch 13/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5667 - loss: 0.6776 - val_accuracy: 0.5181 - val_loss: 0.6909\n",
      "Epoch 14/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5799 - loss: 0.6784 - val_accuracy: 0.5285 - val_loss: 0.6905\n",
      "Epoch 15/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6025 - loss: 0.6698 - val_accuracy: 0.5233 - val_loss: 0.6908\n",
      "Epoch 16/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5775 - loss: 0.6720 - val_accuracy: 0.5285 - val_loss: 0.6904\n",
      "Epoch 17/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6094 - loss: 0.6664 - val_accuracy: 0.5233 - val_loss: 0.6898\n",
      "Epoch 18/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6022 - loss: 0.6664 - val_accuracy: 0.5285 - val_loss: 0.6900\n",
      "Epoch 19/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6324 - loss: 0.6596 - val_accuracy: 0.5233 - val_loss: 0.6892\n",
      "Epoch 20/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5699 - loss: 0.6707 - val_accuracy: 0.5233 - val_loss: 0.6886\n",
      "Epoch 21/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6351 - loss: 0.6594 - val_accuracy: 0.5285 - val_loss: 0.6881\n",
      "Epoch 22/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6175 - loss: 0.6602 - val_accuracy: 0.5285 - val_loss: 0.6882\n",
      "Epoch 23/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6088 - loss: 0.6581 - val_accuracy: 0.5181 - val_loss: 0.6879\n",
      "Epoch 24/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6029 - loss: 0.6594 - val_accuracy: 0.5233 - val_loss: 0.6884\n",
      "Epoch 25/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6281 - loss: 0.6555 - val_accuracy: 0.5337 - val_loss: 0.6876\n",
      "Epoch 26/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6259 - loss: 0.6588 - val_accuracy: 0.5233 - val_loss: 0.6875\n",
      "Epoch 27/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6109 - loss: 0.6556 - val_accuracy: 0.5285 - val_loss: 0.6876\n",
      "Epoch 28/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6287 - loss: 0.6474 - val_accuracy: 0.5492 - val_loss: 0.6861\n",
      "Epoch 29/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6225 - loss: 0.6602 - val_accuracy: 0.5389 - val_loss: 0.6866\n",
      "Epoch 30/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6604 - loss: 0.6340 - val_accuracy: 0.5544 - val_loss: 0.6856\n",
      "Epoch 31/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6155 - loss: 0.6570 - val_accuracy: 0.5440 - val_loss: 0.6852\n",
      "Epoch 32/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6694 - loss: 0.6354 - val_accuracy: 0.5492 - val_loss: 0.6849\n",
      "Epoch 33/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6284 - loss: 0.6460 - val_accuracy: 0.5389 - val_loss: 0.6865\n",
      "Epoch 34/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6423 - loss: 0.6390 - val_accuracy: 0.5492 - val_loss: 0.6863\n",
      "Epoch 35/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6451 - loss: 0.6323 - val_accuracy: 0.5492 - val_loss: 0.6845\n",
      "Epoch 36/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6254 - loss: 0.6418 - val_accuracy: 0.5544 - val_loss: 0.6857\n",
      "Epoch 37/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6378 - loss: 0.6371 - val_accuracy: 0.5596 - val_loss: 0.6845\n",
      "Epoch 38/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6361 - loss: 0.6346 - val_accuracy: 0.5544 - val_loss: 0.6841\n",
      "Epoch 39/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6729 - loss: 0.6157 - val_accuracy: 0.5440 - val_loss: 0.6863\n",
      "Epoch 40/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6383 - loss: 0.6281 - val_accuracy: 0.5596 - val_loss: 0.6844\n",
      "Epoch 41/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6692 - loss: 0.6286 - val_accuracy: 0.5440 - val_loss: 0.6836\n",
      "Epoch 42/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7024 - loss: 0.6126 - val_accuracy: 0.5440 - val_loss: 0.6837\n",
      "Epoch 43/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6754 - loss: 0.6143 - val_accuracy: 0.5492 - val_loss: 0.6830\n",
      "Epoch 44/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6817 - loss: 0.6095 - val_accuracy: 0.5751 - val_loss: 0.6818\n",
      "Epoch 45/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6710 - loss: 0.6212 - val_accuracy: 0.5389 - val_loss: 0.6844\n",
      "Epoch 46/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7109 - loss: 0.5973 - val_accuracy: 0.5389 - val_loss: 0.6841\n",
      "Epoch 47/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6734 - loss: 0.6111 - val_accuracy: 0.5440 - val_loss: 0.6841\n",
      "Epoch 48/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6846 - loss: 0.6034 - val_accuracy: 0.5596 - val_loss: 0.6832\n",
      "Epoch 49/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6939 - loss: 0.6036 - val_accuracy: 0.5699 - val_loss: 0.6842\n",
      "Epoch 50/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7102 - loss: 0.5964 - val_accuracy: 0.5492 - val_loss: 0.6860\n",
      "Epoch 51/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7070 - loss: 0.5931 - val_accuracy: 0.5544 - val_loss: 0.6866\n",
      "Epoch 52/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7052 - loss: 0.6010 - val_accuracy: 0.5492 - val_loss: 0.6876\n",
      "Epoch 53/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7205 - loss: 0.5816 - val_accuracy: 0.5648 - val_loss: 0.6856\n",
      "Epoch 54/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7548 - loss: 0.5635 - val_accuracy: 0.5596 - val_loss: 0.6866\n",
      "Epoch 54: early stopping\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5737 - loss: 0.6763 \n",
      "----------------------------\n",
      "현재 시간 간격: 30일, 스케일링 방법: minmax\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5528 - loss: 0.6877 - val_accuracy: 0.5181 - val_loss: 0.6936\n",
      "Epoch 2/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5459 - loss: 0.6903 - val_accuracy: 0.5181 - val_loss: 0.6933\n",
      "Epoch 3/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5045 - loss: 0.6943 - val_accuracy: 0.5181 - val_loss: 0.6934\n",
      "Epoch 4/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5162 - loss: 0.6921 - val_accuracy: 0.5181 - val_loss: 0.6931\n",
      "Epoch 5/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5178 - loss: 0.6907 - val_accuracy: 0.5181 - val_loss: 0.6927\n",
      "Epoch 6/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5524 - loss: 0.6876 - val_accuracy: 0.5181 - val_loss: 0.6927\n",
      "Epoch 7/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5389 - loss: 0.6892 - val_accuracy: 0.5181 - val_loss: 0.6927\n",
      "Epoch 8/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5128 - loss: 0.6929 - val_accuracy: 0.5181 - val_loss: 0.6927\n",
      "Epoch 9/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5299 - loss: 0.6915 - val_accuracy: 0.5181 - val_loss: 0.6928\n",
      "Epoch 10/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5343 - loss: 0.6892 - val_accuracy: 0.5181 - val_loss: 0.6921\n",
      "Epoch 11/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5406 - loss: 0.6865 - val_accuracy: 0.5181 - val_loss: 0.6919\n",
      "Epoch 12/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5346 - loss: 0.6878 - val_accuracy: 0.5181 - val_loss: 0.6921\n",
      "Epoch 13/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5459 - loss: 0.6878 - val_accuracy: 0.5181 - val_loss: 0.6919\n",
      "Epoch 14/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5499 - loss: 0.6877 - val_accuracy: 0.5233 - val_loss: 0.6919\n",
      "Epoch 15/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5618 - loss: 0.6836 - val_accuracy: 0.5233 - val_loss: 0.6914\n",
      "Epoch 16/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5560 - loss: 0.6858 - val_accuracy: 0.5233 - val_loss: 0.6914\n",
      "Epoch 17/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5329 - loss: 0.6860 - val_accuracy: 0.5233 - val_loss: 0.6915\n",
      "Epoch 18/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5264 - loss: 0.6895 - val_accuracy: 0.5233 - val_loss: 0.6912\n",
      "Epoch 19/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5583 - loss: 0.6829 - val_accuracy: 0.5233 - val_loss: 0.6911\n",
      "Epoch 20/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5232 - loss: 0.6890 - val_accuracy: 0.5181 - val_loss: 0.6910\n",
      "Epoch 21/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5423 - loss: 0.6872 - val_accuracy: 0.5233 - val_loss: 0.6910\n",
      "Epoch 22/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5574 - loss: 0.6831 - val_accuracy: 0.5181 - val_loss: 0.6907\n",
      "Epoch 23/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5328 - loss: 0.6862 - val_accuracy: 0.5233 - val_loss: 0.6914\n",
      "Epoch 24/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5346 - loss: 0.6862 - val_accuracy: 0.5181 - val_loss: 0.6901\n",
      "Epoch 25/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5350 - loss: 0.6891 - val_accuracy: 0.5233 - val_loss: 0.6911\n",
      "Epoch 26/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5525 - loss: 0.6851 - val_accuracy: 0.5181 - val_loss: 0.6908\n",
      "Epoch 27/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5107 - loss: 0.6892 - val_accuracy: 0.5285 - val_loss: 0.6904\n",
      "Epoch 28/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5312 - loss: 0.6874 - val_accuracy: 0.5181 - val_loss: 0.6915\n",
      "Epoch 29/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5452 - loss: 0.6834 - val_accuracy: 0.5337 - val_loss: 0.6896\n",
      "Epoch 30/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5526 - loss: 0.6820 - val_accuracy: 0.5337 - val_loss: 0.6894\n",
      "Epoch 31/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5782 - loss: 0.6814 - val_accuracy: 0.5440 - val_loss: 0.6892\n",
      "Epoch 32/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5520 - loss: 0.6833 - val_accuracy: 0.5440 - val_loss: 0.6893\n",
      "Epoch 33/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5455 - loss: 0.6859 - val_accuracy: 0.5285 - val_loss: 0.6904\n",
      "Epoch 34/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5207 - loss: 0.6836 - val_accuracy: 0.5285 - val_loss: 0.6898\n",
      "Epoch 35/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5678 - loss: 0.6845 - val_accuracy: 0.5130 - val_loss: 0.6885\n",
      "Epoch 36/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5413 - loss: 0.6867 - val_accuracy: 0.5285 - val_loss: 0.6902\n",
      "Epoch 37/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5673 - loss: 0.6828 - val_accuracy: 0.5389 - val_loss: 0.6895\n",
      "Epoch 38/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5529 - loss: 0.6821 - val_accuracy: 0.5130 - val_loss: 0.6883\n",
      "Epoch 39/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5705 - loss: 0.6803 - val_accuracy: 0.5078 - val_loss: 0.6886\n",
      "Epoch 40/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5836 - loss: 0.6784 - val_accuracy: 0.5544 - val_loss: 0.6878\n",
      "Epoch 41/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5777 - loss: 0.6770 - val_accuracy: 0.5233 - val_loss: 0.6879\n",
      "Epoch 42/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5340 - loss: 0.6884 - val_accuracy: 0.5492 - val_loss: 0.6896\n",
      "Epoch 43/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5353 - loss: 0.6825 - val_accuracy: 0.5492 - val_loss: 0.6896\n",
      "Epoch 44/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5784 - loss: 0.6771 - val_accuracy: 0.5181 - val_loss: 0.6888\n",
      "Epoch 45/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5799 - loss: 0.6781 - val_accuracy: 0.5285 - val_loss: 0.6892\n",
      "Epoch 46/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5571 - loss: 0.6834 - val_accuracy: 0.5233 - val_loss: 0.6881\n",
      "Epoch 47/100\n",
      "\u001b[1m 7/97\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5665 - loss: 0.6745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "### SEQUENCE 별 성능\n",
    "def split_xy(dataset, time_steps, y_column):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i + time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "\n",
    "        if y_end_number > len(dataset):\n",
    "            break\n",
    "        tmp_x = dataset.iloc[i:x_end_number, :]\n",
    "        tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "        x.append(tmp_x.values)\n",
    "        y.append(tmp_y.values)\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# 함수 정의: 모델 생성 및 훈련\n",
    "def train_model(X_train, y_train):\n",
    "    # 조기 종료 설정\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    # 모델 구성\n",
    "    model = Sequential([\n",
    "        GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 훈련\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'테스트 손실: {loss}')\n",
    "    print(f'테스트 정확도: {accuracy}')\n",
    "\n",
    "# 시간 간격 및 스케일링 방법 설정\n",
    "time_steps_list = [5,10,15, 30, 60,90]\n",
    "scaling_methods = ['standard', 'minmax']\n",
    "y_column = 1  # 하루 예측\n",
    "results = []\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "    for scaling_method in scaling_methods:\n",
    "        print(f\"현재 시간 간격: {time_steps}일, 스케일링 방법: {scaling_method}\")\n",
    "        X, y = split_xy(data, time_steps, y_column)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=True)\n",
    "        \n",
    "        if scaling_method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling_method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train)\n",
    "        loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "        results.append({'시간 간격': time_steps, '스케일링 방법': scaling_method, '정확도': accuracy})\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9a282",
   "metadata": {},
   "source": [
    "### 초기화함수별 성능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69225e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size :  (4970, 15, 45)\n",
      "y size :  (4970, 1)\n",
      "현재 초기화 함수: GlorotUniform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4984 - loss: 0.6958 - val_accuracy: 0.5075 - val_loss: 0.6935\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5288 - loss: 0.6916 - val_accuracy: 0.5176 - val_loss: 0.6924\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5086 - loss: 0.6931 - val_accuracy: 0.5050 - val_loss: 0.6925\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4945 - loss: 0.6931 - val_accuracy: 0.5101 - val_loss: 0.6932\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5121 - loss: 0.6912 - val_accuracy: 0.5138 - val_loss: 0.6923\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5137 - loss: 0.6936 - val_accuracy: 0.5050 - val_loss: 0.6951\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5197 - loss: 0.6917 - val_accuracy: 0.5138 - val_loss: 0.6921\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5198 - loss: 0.6909 - val_accuracy: 0.5352 - val_loss: 0.6915\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.5162 - loss: 0.6909 - val_accuracy: 0.5113 - val_loss: 0.6937\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5030 - loss: 0.6906 - val_accuracy: 0.5126 - val_loss: 0.6930\n",
      "Epoch 11/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5230 - loss: 0.6900 - val_accuracy: 0.5176 - val_loss: 0.7012\n",
      "Epoch 12/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5177 - loss: 0.6934 - val_accuracy: 0.5063 - val_loss: 0.6927\n",
      "Epoch 13/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4962 - loss: 0.6880 - val_accuracy: 0.5163 - val_loss: 0.6943\n",
      "Epoch 14/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5250 - loss: 0.6877 - val_accuracy: 0.5088 - val_loss: 0.6924\n",
      "Epoch 15/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5164 - loss: 0.6864 - val_accuracy: 0.5188 - val_loss: 0.6933\n",
      "Epoch 16/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5225 - loss: 0.6867 - val_accuracy: 0.5226 - val_loss: 0.6925\n",
      "Epoch 17/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5351 - loss: 0.6847 - val_accuracy: 0.5188 - val_loss: 0.6933\n",
      "Epoch 18/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5247 - loss: 0.6874 - val_accuracy: 0.5264 - val_loss: 0.6919\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5235 - loss: 0.6953\n",
      "테스트 손실: 0.6942774057388306\n",
      "테스트 정확도: 0.5150905251502991\n",
      "----------------------------\n",
      "현재 초기화 함수: GlorotNormal\n",
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4908 - loss: 0.6994 - val_accuracy: 0.5050 - val_loss: 0.6935\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5097 - loss: 0.6938 - val_accuracy: 0.4912 - val_loss: 0.6948\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4886 - loss: 0.6938 - val_accuracy: 0.5101 - val_loss: 0.6926\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5257 - loss: 0.6911 - val_accuracy: 0.5126 - val_loss: 0.6920\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 0.6908 - val_accuracy: 0.5101 - val_loss: 0.6939\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5176 - loss: 0.6916 - val_accuracy: 0.4862 - val_loss: 0.6926\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5077 - loss: 0.6895 - val_accuracy: 0.5151 - val_loss: 0.6972\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5317 - loss: 0.6902 - val_accuracy: 0.5113 - val_loss: 0.6928\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5204 - loss: 0.6915 - val_accuracy: 0.5038 - val_loss: 0.6932\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5177 - loss: 0.6908 - val_accuracy: 0.5113 - val_loss: 0.6930\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5408 - loss: 0.6915\n",
      "테스트 손실: 0.6924669146537781\n",
      "테스트 정확도: 0.5301811099052429\n",
      "----------------------------\n",
      "현재 초기화 함수: HeUniform\n",
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.4901 - loss: 0.7156 - val_accuracy: 0.5025 - val_loss: 0.6978\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.4996 - loss: 0.6942 - val_accuracy: 0.4899 - val_loss: 0.6950\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5059 - loss: 0.6938 - val_accuracy: 0.4862 - val_loss: 0.6960\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5157 - loss: 0.6941 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5010 - loss: 0.6945 - val_accuracy: 0.5201 - val_loss: 0.6966\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5087 - loss: 0.6916 - val_accuracy: 0.5000 - val_loss: 0.6945\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.4992 - loss: 0.6939 - val_accuracy: 0.5013 - val_loss: 0.6944\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5132 - loss: 0.6897 - val_accuracy: 0.5063 - val_loss: 0.6965\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5259 - loss: 0.6879 - val_accuracy: 0.4912 - val_loss: 0.6947\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5089 - loss: 0.6912 - val_accuracy: 0.5126 - val_loss: 0.6948\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5348 - loss: 0.6938\n",
      "테스트 손실: 0.6959764361381531\n",
      "테스트 정확도: 0.5211267471313477\n",
      "----------------------------\n",
      "현재 초기화 함수: HeNormal\n",
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4918 - loss: 0.7144 - val_accuracy: 0.5101 - val_loss: 0.7009\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5156 - loss: 0.6951 - val_accuracy: 0.5013 - val_loss: 0.6933\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5079 - loss: 0.6935 - val_accuracy: 0.4987 - val_loss: 0.6948\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5148 - loss: 0.6926 - val_accuracy: 0.5050 - val_loss: 0.6932\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5070 - loss: 0.6924 - val_accuracy: 0.5038 - val_loss: 0.6929\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5216 - loss: 0.6922 - val_accuracy: 0.4987 - val_loss: 0.6938\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5197 - loss: 0.6909 - val_accuracy: 0.5088 - val_loss: 0.6934\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5236 - loss: 0.6904 - val_accuracy: 0.5138 - val_loss: 0.6924\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.4958 - loss: 0.6918 - val_accuracy: 0.5188 - val_loss: 0.6923\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.5176 - val_loss: 0.6936\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5114 - loss: 0.6953\n",
      "테스트 손실: 0.6968309879302979\n",
      "테스트 정확도: 0.5080482959747314\n",
      "----------------------------\n",
      "현재 초기화 함수: RandomUniform\n",
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.4862 - loss: 0.6933 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5086 - loss: 0.6931 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5078 - loss: 0.6931 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5061 - loss: 0.6931 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5004 - loss: 0.6933 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5020 - loss: 0.6932 - val_accuracy: 0.5088 - val_loss: 0.6930\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5093 - loss: 0.6930 - val_accuracy: 0.5101 - val_loss: 0.6929\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5134 - loss: 0.6928 - val_accuracy: 0.5101 - val_loss: 0.6928\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5251 - loss: 0.6923 - val_accuracy: 0.5038 - val_loss: 0.6933\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5105 - loss: 0.6932 - val_accuracy: 0.5088 - val_loss: 0.6928\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5402 - loss: 0.6926\n",
      "테스트 손실: 0.6928289532661438\n",
      "테스트 정확도: 0.5241448879241943\n",
      "----------------------------\n",
      "현재 초기화 함수: RandomNormal\n",
      "Epoch 1/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4859 - loss: 0.6939 - val_accuracy: 0.5101 - val_loss: 0.6930\n",
      "Epoch 2/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4972 - loss: 0.6931 - val_accuracy: 0.5025 - val_loss: 0.6930\n",
      "Epoch 3/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5016 - loss: 0.6932 - val_accuracy: 0.5101 - val_loss: 0.6929\n",
      "Epoch 4/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5116 - loss: 0.6929 - val_accuracy: 0.5050 - val_loss: 0.6930\n",
      "Epoch 5/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5095 - loss: 0.6922 - val_accuracy: 0.5101 - val_loss: 0.6926\n",
      "Epoch 6/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.4958 - loss: 0.6933 - val_accuracy: 0.5088 - val_loss: 0.6926\n",
      "Epoch 7/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5201 - loss: 0.6920 - val_accuracy: 0.5088 - val_loss: 0.6926\n",
      "Epoch 8/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5186 - loss: 0.6921 - val_accuracy: 0.5088 - val_loss: 0.6925\n",
      "Epoch 9/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5271 - loss: 0.6914 - val_accuracy: 0.5063 - val_loss: 0.6936\n",
      "Epoch 10/100\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5281 - loss: 0.6908 - val_accuracy: 0.5101 - val_loss: 0.6924\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5431 - loss: 0.6923\n",
      "테스트 손실: 0.6926586627960205\n",
      "테스트 정확도: 0.5251508951187134\n",
      "----------------------------\n",
      "가장 좋은 초기화 함수: GlorotNormal, 정확도: 0.5301811099052429\n"
     ]
    }
   ],
   "source": [
    "def split_xy(dataset, time_steps, y_column):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i + time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "\n",
    "        if y_end_number > len(dataset):\n",
    "            break\n",
    "        tmp_x = dataset.iloc[i:x_end_number, :] \n",
    "        tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "        x.append(tmp_x.values)  \n",
    "        y.append(tmp_y.values)  \n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "X, y = split_xy(data, 15, PRED_SIZE)\n",
    "# print(X[0],\"\\n\", y[0])\n",
    "print(\"X size : \", X.shape)\n",
    "print(\"y size : \", y.shape)\n",
    "\n",
    "def split_and_standardize(dataset, time_steps, y_column):\n",
    "    X, y = split_xy(dataset, time_steps, y_column)\n",
    "    X_standardized = []\n",
    "    for seq in X:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_seq = scaler.fit_transform(seq)\n",
    "        X_standardized.append(scaled_seq)\n",
    "    return np.array(X_standardized), y\n",
    "X_std, y = split_and_standardize(data, time_steps=15, y_column=PRED_SIZE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2, shuffle=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal, HeUniform, HeNormal, RandomUniform, RandomNormal\n",
    "\n",
    "\n",
    "# EarlyStopping 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# 초기화 함수 리스트\n",
    "initializers = [GlorotUniform(), GlorotNormal(), HeUniform(), HeNormal(), RandomUniform(), RandomNormal()]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_initializer = None\n",
    "\n",
    "for initializer in initializers:\n",
    "    print(f\"현재 초기화 함수: {initializer.__class__.__name__}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(64, activation='relu', kernel_initializer=initializer),\n",
    "        Dense(32, activation='relu', kernel_initializer=initializer),\n",
    "        Dense(16, activation='relu', kernel_initializer=initializer),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'테스트 손실: {loss}')\n",
    "    print(f'테스트 정확도: {accuracy}')\n",
    "    print(\"----------------------------\")\n",
    "    \n",
    "    # 가장 높은 정확도를 가진 초기화 함수 선택\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_initializer = initializer\n",
    "\n",
    "print(f\"가장 좋은 초기화 함수: {best_initializer.__class__.__name__}, 정확도: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b57ad1",
   "metadata": {},
   "source": [
    "### 층 개수 별 성능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d22ec47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "최적의 Dense 층 개수: 3, 최고 검증 정확도: 0.5804020166397095\n",
      "Dense 층 개수: 1, 검증 정확도: 0.5653266310691833\n",
      "Dense 층 개수: 2, 검증 정확도: 0.5565326809883118\n",
      "Dense 층 개수: 3, 검증 정확도: 0.5804020166397095\n",
      "Dense 층 개수: 4, 검증 정확도: 0.5766331553459167\n",
      "Dense 층 개수: 5, 검증 정확도: 0.4899497628211975\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "best_precision = 0\n",
    "best_num_dense_layers = 0\n",
    "precision_results = {}\n",
    "\n",
    "# 시도할 Dense 층 개수 범위\n",
    "num_dense_layers_range = range(1, 6)  \n",
    "\n",
    "for num_dense_layers in num_dense_layers_range:\n",
    "    model = Sequential([\n",
    "        LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    ])\n",
    "    \n",
    "    for _ in range(num_dense_layers):\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=8, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    precision = precision_score(y_test, y_pred_binary)\n",
    "    precision_results[num_dense_layers] = precision\n",
    "    \n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_num_dense_layers = num_dense_layers\n",
    "\n",
    "print(f'최적의 Dense 층 개수: {best_num_dense_layers}, 최고 검증 정밀도: {best_precision}')\n",
    "\n",
    "# 각 층 개수별 정확도 출력\n",
    "for num_dense_layers, precision in precision_results.items():\n",
    "    print(f'Dense 층 개수: {num_dense_layers}, 검증 정밀도: {precision}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3012c4c",
   "metadata": {},
   "source": [
    "### l2 정규화 방식\n",
    "- 논문을 바탕으로 정규화 구성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ebea3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 값: 0.003\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5430 - loss: 6.0864\n",
      "L2 정규화 - 테스트 손실: 6.1332597732543945, 테스트 정확도: 0.5251508951187134\n",
      "----------------------------\n",
      "L2 값: 0.004\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5401 - loss: 9.5307\n",
      "L2 정규화 - 테스트 손실: 9.47807788848877, 테스트 정확도: 0.5221328139305115\n",
      "----------------------------\n",
      "L2 정규화 결과:\n",
      "{'L2_0.003': {'loss': 6.1332597732543945, 'accuracy': 0.5251508951187134}, 'L2_0.004': {'loss': 9.47807788848877, 'accuracy': 0.5221328139305115}}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "initializer = GlorotNormal()\n",
    "results_l2 = {}\n",
    "\n",
    "activity_regularizer = regularizers.l2(0.1)\n",
    "\n",
    "for l2_value in [0.003, 0.004]:\n",
    "    print(f\"L2 값: {l2_value}\")\n",
    "\n",
    "    # 모델 생성\n",
    "    model_l2 = Sequential([\n",
    "        GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(256, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2_value), activity_regularizer=activity_regularizer),\n",
    "        Dense(128, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2_value), activity_regularizer=activity_regularizer),\n",
    "        Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2_value), activity_regularizer=activity_regularizer),\n",
    "        Dense(32, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(l2_value), activity_regularizer=activity_regularizer),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model_l2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history_l2 = model_l2.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=0)    \n",
    "    loss_l2, accuracy_l2 = model_l2.evaluate(X_test, y_test)\n",
    "    print(f'L2 정규화 - 테스트 손실: {loss_l2}, 테스트 정확도: {accuracy_l2}')\n",
    "\n",
    "    results_l2[f'L2_{l2_value}'] = {'loss': loss_l2, 'accuracy': accuracy_l2}\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"L2 정규화 결과:\")\n",
    "print(results_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b217940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - accuracy: 0.5002 - loss: 0.9462 - val_accuracy: 0.5163 - val_loss: 0.8151\n",
      "Epoch 2/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.5187 - loss: 0.7941 - val_accuracy: 0.5088 - val_loss: 0.7484\n",
      "Epoch 3/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.5296 - loss: 0.7386 - val_accuracy: 0.4975 - val_loss: 0.7216\n",
      "Epoch 4/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5223 - loss: 0.7166 - val_accuracy: 0.5314 - val_loss: 0.7089\n",
      "Epoch 5/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5317 - loss: 0.7068 - val_accuracy: 0.5113 - val_loss: 0.7029\n",
      "Epoch 6/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5120 - loss: 0.7018 - val_accuracy: 0.5327 - val_loss: 0.6990\n",
      "Epoch 7/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.5313 - loss: 0.6978 - val_accuracy: 0.5415 - val_loss: 0.6971\n",
      "Epoch 8/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.5319 - loss: 0.6967 - val_accuracy: 0.5339 - val_loss: 0.6959\n",
      "Epoch 9/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5398 - loss: 0.6945 - val_accuracy: 0.5477 - val_loss: 0.6948\n",
      "Epoch 10/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5653 - loss: 0.6930 - val_accuracy: 0.5452 - val_loss: 0.6941\n",
      "Epoch 11/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.5490 - loss: 0.6917 - val_accuracy: 0.5364 - val_loss: 0.6936\n",
      "Epoch 12/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5751 - loss: 0.6875 - val_accuracy: 0.5389 - val_loss: 0.6941\n",
      "Epoch 13/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5573 - loss: 0.6878 - val_accuracy: 0.5490 - val_loss: 0.6955\n",
      "Epoch 14/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5712 - loss: 0.6865 - val_accuracy: 0.5415 - val_loss: 0.6942\n",
      "Epoch 15/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.5737 - loss: 0.6832 - val_accuracy: 0.5302 - val_loss: 0.6996\n",
      "Epoch 16/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.5621 - loss: 0.6868 - val_accuracy: 0.5565 - val_loss: 0.6936\n",
      "Epoch 17/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.5804 - loss: 0.6806 - val_accuracy: 0.5528 - val_loss: 0.6953\n",
      "Epoch 18/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5839 - loss: 0.6815 - val_accuracy: 0.5590 - val_loss: 0.6967\n",
      "Epoch 19/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5835 - loss: 0.6807 - val_accuracy: 0.5490 - val_loss: 0.6983\n",
      "Epoch 20/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.5983 - loss: 0.6736 - val_accuracy: 0.5528 - val_loss: 0.6962\n",
      "Epoch 21/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.6044 - loss: 0.6733 - val_accuracy: 0.5590 - val_loss: 0.6972\n",
      "Epoch 22/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5950 - loss: 0.6750 - val_accuracy: 0.5704 - val_loss: 0.6978\n",
      "Epoch 23/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5989 - loss: 0.6725 - val_accuracy: 0.5565 - val_loss: 0.7006\n",
      "Epoch 24/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.6003 - loss: 0.6697 - val_accuracy: 0.5402 - val_loss: 0.7034\n",
      "Epoch 25/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.6081 - loss: 0.6659 - val_accuracy: 0.5540 - val_loss: 0.7003\n",
      "Epoch 26/200\n",
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.6035 - loss: 0.6694 - val_accuracy: 0.5565 - val_loss: 0.7049\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6041 - loss: 0.6820 \n",
      "Batch Size: 1, Learning Rate: 0.0001, Test Accuracy: 0.5814889073371887\n",
      "Epoch 1/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.4907 - loss: 0.9822 - val_accuracy: 0.5226 - val_loss: 0.9077\n",
      "Epoch 2/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5062 - loss: 0.8913 - val_accuracy: 0.5163 - val_loss: 0.8473\n",
      "Epoch 3/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5137 - loss: 0.8369 - val_accuracy: 0.5075 - val_loss: 0.8082\n",
      "Epoch 4/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5155 - loss: 0.8008 - val_accuracy: 0.5389 - val_loss: 0.7814\n",
      "Epoch 5/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5248 - loss: 0.7761 - val_accuracy: 0.5201 - val_loss: 0.7621\n",
      "Epoch 6/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.5221 - loss: 0.7583 - val_accuracy: 0.5126 - val_loss: 0.7474\n",
      "Epoch 7/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5258 - loss: 0.7452 - val_accuracy: 0.5251 - val_loss: 0.7366\n",
      "Epoch 8/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5184 - loss: 0.7347 - val_accuracy: 0.5176 - val_loss: 0.7277\n",
      "Epoch 9/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5126 - loss: 0.7268 - val_accuracy: 0.5101 - val_loss: 0.7214\n",
      "Epoch 10/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5173 - loss: 0.7202 - val_accuracy: 0.5113 - val_loss: 0.7155\n",
      "Epoch 11/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5325 - loss: 0.7141 - val_accuracy: 0.5327 - val_loss: 0.7111\n",
      "Epoch 12/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5328 - loss: 0.7102 - val_accuracy: 0.5214 - val_loss: 0.7078\n",
      "Epoch 13/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5236 - loss: 0.7074 - val_accuracy: 0.5314 - val_loss: 0.7051\n",
      "Epoch 14/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5342 - loss: 0.7045 - val_accuracy: 0.5339 - val_loss: 0.7029\n",
      "Epoch 15/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5324 - loss: 0.7029 - val_accuracy: 0.5364 - val_loss: 0.7013\n",
      "Epoch 16/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5348 - loss: 0.7008 - val_accuracy: 0.5126 - val_loss: 0.7002\n",
      "Epoch 17/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5359 - loss: 0.6988 - val_accuracy: 0.5616 - val_loss: 0.6989\n",
      "Epoch 18/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5495 - loss: 0.6983 - val_accuracy: 0.5503 - val_loss: 0.6980\n",
      "Epoch 19/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5410 - loss: 0.6982 - val_accuracy: 0.5515 - val_loss: 0.6972\n",
      "Epoch 20/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5715 - loss: 0.6950 - val_accuracy: 0.5402 - val_loss: 0.6971\n",
      "Epoch 21/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.5702 - loss: 0.6942 - val_accuracy: 0.5327 - val_loss: 0.6964\n",
      "Epoch 22/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5672 - loss: 0.6931 - val_accuracy: 0.5590 - val_loss: 0.6958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5541 - loss: 0.6923 - val_accuracy: 0.5603 - val_loss: 0.6957\n",
      "Epoch 24/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5587 - loss: 0.6906 - val_accuracy: 0.5641 - val_loss: 0.6959\n",
      "Epoch 25/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5837 - loss: 0.6888 - val_accuracy: 0.5540 - val_loss: 0.6960\n",
      "Epoch 26/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5721 - loss: 0.6876 - val_accuracy: 0.5628 - val_loss: 0.6964\n",
      "Epoch 27/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 0.6846 - val_accuracy: 0.5402 - val_loss: 0.6988\n",
      "Epoch 28/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5961 - loss: 0.6801 - val_accuracy: 0.5515 - val_loss: 0.6981\n",
      "Epoch 29/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5836 - loss: 0.6827 - val_accuracy: 0.5603 - val_loss: 0.6983\n",
      "Epoch 30/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5876 - loss: 0.6810 - val_accuracy: 0.5503 - val_loss: 0.6992\n",
      "Epoch 31/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5885 - loss: 0.6784 - val_accuracy: 0.5503 - val_loss: 0.7014\n",
      "Epoch 32/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5968 - loss: 0.6835 - val_accuracy: 0.5603 - val_loss: 0.7014\n",
      "Epoch 33/200\n",
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5885 - loss: 0.6786 - val_accuracy: 0.5503 - val_loss: 0.7033\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5610 - loss: 0.6918 \n",
      "Batch Size: 4, Learning Rate: 0.0001, Test Accuracy: 0.5543259382247925\n",
      "Epoch 1/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.5238 - loss: 0.9946 - val_accuracy: 0.5113 - val_loss: 0.9444\n",
      "Epoch 2/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5077 - loss: 0.9312 - val_accuracy: 0.5176 - val_loss: 0.8913\n",
      "Epoch 3/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5198 - loss: 0.8800 - val_accuracy: 0.5276 - val_loss: 0.8504\n",
      "Epoch 4/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5118 - loss: 0.8423 - val_accuracy: 0.5038 - val_loss: 0.8184\n",
      "Epoch 5/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5231 - loss: 0.8114 - val_accuracy: 0.5101 - val_loss: 0.7937\n",
      "Epoch 6/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5266 - loss: 0.7885 - val_accuracy: 0.5289 - val_loss: 0.7745\n",
      "Epoch 7/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5263 - loss: 0.7707 - val_accuracy: 0.5075 - val_loss: 0.7598\n",
      "Epoch 8/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5247 - loss: 0.7559 - val_accuracy: 0.5402 - val_loss: 0.7482\n",
      "Epoch 9/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5394 - loss: 0.7452 - val_accuracy: 0.5214 - val_loss: 0.7391\n",
      "Epoch 10/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5286 - loss: 0.7367 - val_accuracy: 0.5377 - val_loss: 0.7321\n",
      "Epoch 11/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5271 - loss: 0.7303 - val_accuracy: 0.5452 - val_loss: 0.7264\n",
      "Epoch 12/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5302 - loss: 0.7245 - val_accuracy: 0.5427 - val_loss: 0.7219\n",
      "Epoch 13/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5344 - loss: 0.7202 - val_accuracy: 0.5440 - val_loss: 0.7183\n",
      "Epoch 14/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5474 - loss: 0.7161 - val_accuracy: 0.5352 - val_loss: 0.7152\n",
      "Epoch 15/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5331 - loss: 0.7146 - val_accuracy: 0.5214 - val_loss: 0.7130\n",
      "Epoch 16/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5246 - loss: 0.7117 - val_accuracy: 0.5314 - val_loss: 0.7106\n",
      "Epoch 17/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5457 - loss: 0.7078 - val_accuracy: 0.5327 - val_loss: 0.7089\n",
      "Epoch 18/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5364 - loss: 0.7073 - val_accuracy: 0.5251 - val_loss: 0.7074\n",
      "Epoch 19/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5365 - loss: 0.7056 - val_accuracy: 0.5402 - val_loss: 0.7059\n",
      "Epoch 20/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5522 - loss: 0.7025 - val_accuracy: 0.5364 - val_loss: 0.7048\n",
      "Epoch 21/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5417 - loss: 0.7036 - val_accuracy: 0.5528 - val_loss: 0.7031\n",
      "Epoch 22/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5610 - loss: 0.6996 - val_accuracy: 0.5603 - val_loss: 0.7019\n",
      "Epoch 23/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5524 - loss: 0.7006 - val_accuracy: 0.5678 - val_loss: 0.7010\n",
      "Epoch 24/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5733 - loss: 0.6969 - val_accuracy: 0.5616 - val_loss: 0.7007\n",
      "Epoch 25/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5665 - loss: 0.6965 - val_accuracy: 0.5440 - val_loss: 0.7015\n",
      "Epoch 26/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5539 - loss: 0.6965 - val_accuracy: 0.5817 - val_loss: 0.6990\n",
      "Epoch 27/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5677 - loss: 0.6933 - val_accuracy: 0.5716 - val_loss: 0.6983\n",
      "Epoch 28/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5928 - loss: 0.6878 - val_accuracy: 0.5503 - val_loss: 0.6988\n",
      "Epoch 29/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5545 - loss: 0.6939 - val_accuracy: 0.5465 - val_loss: 0.6988\n",
      "Epoch 30/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5708 - loss: 0.6881 - val_accuracy: 0.5628 - val_loss: 0.7001\n",
      "Epoch 31/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.5824 - loss: 0.6863 - val_accuracy: 0.5729 - val_loss: 0.6982\n",
      "Epoch 32/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5838 - loss: 0.6865 - val_accuracy: 0.5603 - val_loss: 0.6982\n",
      "Epoch 33/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5859 - loss: 0.6845 - val_accuracy: 0.5779 - val_loss: 0.6987\n",
      "Epoch 34/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5767 - loss: 0.6854 - val_accuracy: 0.5590 - val_loss: 0.6987\n",
      "Epoch 35/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5795 - loss: 0.6864 - val_accuracy: 0.5716 - val_loss: 0.7005\n",
      "Epoch 36/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5803 - loss: 0.6855 - val_accuracy: 0.5528 - val_loss: 0.6996\n",
      "Epoch 37/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5701 - loss: 0.6834 - val_accuracy: 0.5691 - val_loss: 0.7010\n",
      "Epoch 38/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5745 - loss: 0.6856 - val_accuracy: 0.5528 - val_loss: 0.7008\n",
      "Epoch 39/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5745 - loss: 0.6852 - val_accuracy: 0.5553 - val_loss: 0.7015\n",
      "Epoch 40/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5916 - loss: 0.6811 - val_accuracy: 0.5490 - val_loss: 0.7015\n",
      "Epoch 41/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5994 - loss: 0.6809 - val_accuracy: 0.5590 - val_loss: 0.7029\n",
      "Epoch 42/200\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5807 - loss: 0.6828 - val_accuracy: 0.5490 - val_loss: 0.7023\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5988 - loss: 0.6867\n",
      "Batch Size: 8, Learning Rate: 0.0001, Test Accuracy: 0.5895372033119202\n",
      "Epoch 1/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.4872 - loss: 1.0090 - val_accuracy: 0.5025 - val_loss: 0.9812\n",
      "Epoch 2/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5166 - loss: 0.9722 - val_accuracy: 0.5050 - val_loss: 0.9502\n",
      "Epoch 3/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5255 - loss: 0.9420 - val_accuracy: 0.5163 - val_loss: 0.9228\n",
      "Epoch 4/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5313 - loss: 0.9148 - val_accuracy: 0.5113 - val_loss: 0.8987\n",
      "Epoch 5/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5301 - loss: 0.8918 - val_accuracy: 0.5151 - val_loss: 0.8774\n",
      "Epoch 6/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5398 - loss: 0.8703 - val_accuracy: 0.5289 - val_loss: 0.8587\n",
      "Epoch 7/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5508 - loss: 0.8518 - val_accuracy: 0.5226 - val_loss: 0.8423\n",
      "Epoch 8/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5440 - loss: 0.8369 - val_accuracy: 0.5214 - val_loss: 0.8272\n",
      "Epoch 9/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5340 - loss: 0.8221 - val_accuracy: 0.5188 - val_loss: 0.8142\n",
      "Epoch 10/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5528 - loss: 0.8090 - val_accuracy: 0.5226 - val_loss: 0.8031\n",
      "Epoch 11/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5376 - loss: 0.7996 - val_accuracy: 0.5314 - val_loss: 0.7920\n",
      "Epoch 12/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5308 - loss: 0.7894 - val_accuracy: 0.5214 - val_loss: 0.7830\n",
      "Epoch 13/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5617 - loss: 0.7775 - val_accuracy: 0.5264 - val_loss: 0.7748\n",
      "Epoch 14/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5362 - loss: 0.7722 - val_accuracy: 0.5302 - val_loss: 0.7679\n",
      "Epoch 15/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5393 - loss: 0.7659 - val_accuracy: 0.5440 - val_loss: 0.7611\n",
      "Epoch 16/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5567 - loss: 0.7572 - val_accuracy: 0.5289 - val_loss: 0.7568\n",
      "Epoch 17/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5554 - loss: 0.7517 - val_accuracy: 0.5465 - val_loss: 0.7504\n",
      "Epoch 18/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5456 - loss: 0.7467 - val_accuracy: 0.5352 - val_loss: 0.7463\n",
      "Epoch 19/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5397 - loss: 0.7442 - val_accuracy: 0.5503 - val_loss: 0.7416\n",
      "Epoch 20/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5620 - loss: 0.7387 - val_accuracy: 0.5515 - val_loss: 0.7383\n",
      "Epoch 21/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5542 - loss: 0.7349 - val_accuracy: 0.5490 - val_loss: 0.7355\n",
      "Epoch 22/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5556 - loss: 0.7327 - val_accuracy: 0.5364 - val_loss: 0.7332\n",
      "Epoch 23/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5560 - loss: 0.7300 - val_accuracy: 0.5553 - val_loss: 0.7296\n",
      "Epoch 24/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5876 - loss: 0.7240 - val_accuracy: 0.5528 - val_loss: 0.7276\n",
      "Epoch 25/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5785 - loss: 0.7221 - val_accuracy: 0.5616 - val_loss: 0.7247\n",
      "Epoch 26/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5581 - loss: 0.7206 - val_accuracy: 0.5616 - val_loss: 0.7228\n",
      "Epoch 27/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5918 - loss: 0.7164 - val_accuracy: 0.5704 - val_loss: 0.7217\n",
      "Epoch 28/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5777 - loss: 0.7135 - val_accuracy: 0.5641 - val_loss: 0.7199\n",
      "Epoch 29/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5454 - loss: 0.7162 - val_accuracy: 0.5590 - val_loss: 0.7184\n",
      "Epoch 30/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5922 - loss: 0.7107 - val_accuracy: 0.5729 - val_loss: 0.7170\n",
      "Epoch 31/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5723 - loss: 0.7113 - val_accuracy: 0.5779 - val_loss: 0.7167\n",
      "Epoch 32/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5854 - loss: 0.7077 - val_accuracy: 0.5452 - val_loss: 0.7173\n",
      "Epoch 33/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5721 - loss: 0.7065 - val_accuracy: 0.5704 - val_loss: 0.7150\n",
      "Epoch 34/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5867 - loss: 0.7040 - val_accuracy: 0.5716 - val_loss: 0.7139\n",
      "Epoch 35/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5783 - loss: 0.7059 - val_accuracy: 0.5766 - val_loss: 0.7136\n",
      "Epoch 36/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5867 - loss: 0.7016 - val_accuracy: 0.5653 - val_loss: 0.7137\n",
      "Epoch 37/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5808 - loss: 0.7016 - val_accuracy: 0.5565 - val_loss: 0.7141\n",
      "Epoch 38/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5800 - loss: 0.6994 - val_accuracy: 0.5704 - val_loss: 0.7128\n",
      "Epoch 39/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5866 - loss: 0.6965 - val_accuracy: 0.5716 - val_loss: 0.7122\n",
      "Epoch 40/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6005 - loss: 0.6927 - val_accuracy: 0.5804 - val_loss: 0.7120\n",
      "Epoch 41/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5888 - loss: 0.6991 - val_accuracy: 0.5653 - val_loss: 0.7118\n",
      "Epoch 42/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.6054 - loss: 0.6933 - val_accuracy: 0.5653 - val_loss: 0.7118\n",
      "Epoch 43/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5868 - loss: 0.6960 - val_accuracy: 0.5716 - val_loss: 0.7130\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5977 - loss: 0.6955 - val_accuracy: 0.5641 - val_loss: 0.7119\n",
      "Epoch 45/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5854 - loss: 0.6958 - val_accuracy: 0.5616 - val_loss: 0.7135\n",
      "Epoch 46/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5973 - loss: 0.6900 - val_accuracy: 0.5641 - val_loss: 0.7119\n",
      "Epoch 47/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5935 - loss: 0.6947 - val_accuracy: 0.5716 - val_loss: 0.7120\n",
      "Epoch 48/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5937 - loss: 0.6884 - val_accuracy: 0.5678 - val_loss: 0.7130\n",
      "Epoch 49/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6035 - loss: 0.6857 - val_accuracy: 0.5704 - val_loss: 0.7130\n",
      "Epoch 50/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5971 - loss: 0.6886 - val_accuracy: 0.5528 - val_loss: 0.7148\n",
      "Epoch 51/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6035 - loss: 0.6853 - val_accuracy: 0.5603 - val_loss: 0.7121\n",
      "Epoch 52/200\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6057 - loss: 0.6845 - val_accuracy: 0.5653 - val_loss: 0.7145\n",
      "Epoch 52: early stopping\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5772 - loss: 0.7000\n",
      "Batch Size: 16, Learning Rate: 0.0001, Test Accuracy: 0.5754527449607849\n",
      "Epoch 1/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.5151 - loss: 1.0136 - val_accuracy: 0.5188 - val_loss: 0.9959\n",
      "Epoch 2/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5161 - loss: 0.9893 - val_accuracy: 0.5302 - val_loss: 0.9731\n",
      "Epoch 3/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5212 - loss: 0.9663 - val_accuracy: 0.5226 - val_loss: 0.9520\n",
      "Epoch 4/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5225 - loss: 0.9460 - val_accuracy: 0.5113 - val_loss: 0.9327\n",
      "Epoch 5/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5261 - loss: 0.9269 - val_accuracy: 0.5088 - val_loss: 0.9148\n",
      "Epoch 6/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5263 - loss: 0.9085 - val_accuracy: 0.5113 - val_loss: 0.8984\n",
      "Epoch 7/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5285 - loss: 0.8924 - val_accuracy: 0.4887 - val_loss: 0.8835\n",
      "Epoch 8/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5320 - loss: 0.8779 - val_accuracy: 0.4937 - val_loss: 0.8694\n",
      "Epoch 9/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5339 - loss: 0.8634 - val_accuracy: 0.4925 - val_loss: 0.8564\n",
      "Epoch 10/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5402 - loss: 0.8504 - val_accuracy: 0.5126 - val_loss: 0.8445\n",
      "Epoch 11/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5317 - loss: 0.8383 - val_accuracy: 0.5226 - val_loss: 0.8336\n",
      "Epoch 12/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5568 - loss: 0.8271 - val_accuracy: 0.4950 - val_loss: 0.8232\n",
      "Epoch 13/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5404 - loss: 0.8176 - val_accuracy: 0.5126 - val_loss: 0.8138\n",
      "Epoch 14/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5320 - loss: 0.8091 - val_accuracy: 0.5113 - val_loss: 0.8050\n",
      "Epoch 15/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5373 - loss: 0.7999 - val_accuracy: 0.5013 - val_loss: 0.7967\n",
      "Epoch 16/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5358 - loss: 0.7915 - val_accuracy: 0.5339 - val_loss: 0.7895\n",
      "Epoch 17/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5474 - loss: 0.7840 - val_accuracy: 0.5101 - val_loss: 0.7822\n",
      "Epoch 18/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5484 - loss: 0.7764 - val_accuracy: 0.5088 - val_loss: 0.7759\n",
      "Epoch 19/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5410 - loss: 0.7704 - val_accuracy: 0.5163 - val_loss: 0.7700\n",
      "Epoch 20/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5463 - loss: 0.7648 - val_accuracy: 0.5063 - val_loss: 0.7644\n",
      "Epoch 21/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5678 - loss: 0.7574 - val_accuracy: 0.5477 - val_loss: 0.7598\n",
      "Epoch 22/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5494 - loss: 0.7543 - val_accuracy: 0.5276 - val_loss: 0.7546\n",
      "Epoch 23/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5564 - loss: 0.7496 - val_accuracy: 0.5264 - val_loss: 0.7503\n",
      "Epoch 24/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5470 - loss: 0.7464 - val_accuracy: 0.5188 - val_loss: 0.7464\n",
      "Epoch 25/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5507 - loss: 0.7414 - val_accuracy: 0.5339 - val_loss: 0.7429\n",
      "Epoch 26/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5493 - loss: 0.7389 - val_accuracy: 0.5239 - val_loss: 0.7392\n",
      "Epoch 27/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5594 - loss: 0.7324 - val_accuracy: 0.5440 - val_loss: 0.7368\n",
      "Epoch 28/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5618 - loss: 0.7312 - val_accuracy: 0.5427 - val_loss: 0.7335\n",
      "Epoch 29/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5698 - loss: 0.7272 - val_accuracy: 0.5289 - val_loss: 0.7307\n",
      "Epoch 30/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5577 - loss: 0.7238 - val_accuracy: 0.5276 - val_loss: 0.7281\n",
      "Epoch 31/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5535 - loss: 0.7224 - val_accuracy: 0.5490 - val_loss: 0.7264\n",
      "Epoch 32/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5688 - loss: 0.7196 - val_accuracy: 0.5540 - val_loss: 0.7243\n",
      "Epoch 33/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5795 - loss: 0.7168 - val_accuracy: 0.5515 - val_loss: 0.7230\n",
      "Epoch 34/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5594 - loss: 0.7159 - val_accuracy: 0.5540 - val_loss: 0.7213\n",
      "Epoch 35/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5590 - loss: 0.7164 - val_accuracy: 0.5603 - val_loss: 0.7194\n",
      "Epoch 36/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5687 - loss: 0.7102 - val_accuracy: 0.5565 - val_loss: 0.7181\n",
      "Epoch 37/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5645 - loss: 0.7122 - val_accuracy: 0.5553 - val_loss: 0.7170\n",
      "Epoch 38/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5815 - loss: 0.7075 - val_accuracy: 0.5503 - val_loss: 0.7152\n",
      "Epoch 39/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5700 - loss: 0.7074 - val_accuracy: 0.5339 - val_loss: 0.7144\n",
      "Epoch 40/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5872 - loss: 0.7039 - val_accuracy: 0.5578 - val_loss: 0.7136\n",
      "Epoch 41/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5698 - loss: 0.7051 - val_accuracy: 0.5678 - val_loss: 0.7128\n",
      "Epoch 42/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5591 - loss: 0.7048 - val_accuracy: 0.5590 - val_loss: 0.7134\n",
      "Epoch 43/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5818 - loss: 0.7027 - val_accuracy: 0.5440 - val_loss: 0.7114\n",
      "Epoch 44/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5715 - loss: 0.7019 - val_accuracy: 0.5641 - val_loss: 0.7113\n",
      "Epoch 45/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5828 - loss: 0.6995 - val_accuracy: 0.5364 - val_loss: 0.7101\n",
      "Epoch 46/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5692 - loss: 0.6980 - val_accuracy: 0.5528 - val_loss: 0.7096\n",
      "Epoch 47/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5570 - loss: 0.7009 - val_accuracy: 0.5427 - val_loss: 0.7094\n",
      "Epoch 48/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5978 - loss: 0.6933 - val_accuracy: 0.5515 - val_loss: 0.7088\n",
      "Epoch 49/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5847 - loss: 0.6960 - val_accuracy: 0.5553 - val_loss: 0.7097\n",
      "Epoch 50/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5826 - loss: 0.6945 - val_accuracy: 0.5528 - val_loss: 0.7091\n",
      "Epoch 51/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5764 - loss: 0.6951 - val_accuracy: 0.5553 - val_loss: 0.7088\n",
      "Epoch 52/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5845 - loss: 0.6935 - val_accuracy: 0.5515 - val_loss: 0.7080\n",
      "Epoch 53/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5837 - loss: 0.6933 - val_accuracy: 0.5503 - val_loss: 0.7096\n",
      "Epoch 54/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5882 - loss: 0.6939 - val_accuracy: 0.5565 - val_loss: 0.7075\n",
      "Epoch 55/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5918 - loss: 0.6910 - val_accuracy: 0.5565 - val_loss: 0.7076\n",
      "Epoch 56/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5904 - loss: 0.6889 - val_accuracy: 0.5603 - val_loss: 0.7073\n",
      "Epoch 57/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5910 - loss: 0.6891 - val_accuracy: 0.5440 - val_loss: 0.7094\n",
      "Epoch 58/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5869 - loss: 0.6921 - val_accuracy: 0.5578 - val_loss: 0.7074\n",
      "Epoch 59/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6078 - loss: 0.6861 - val_accuracy: 0.5377 - val_loss: 0.7101\n",
      "Epoch 60/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5921 - loss: 0.6903 - val_accuracy: 0.5578 - val_loss: 0.7070\n",
      "Epoch 61/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5860 - loss: 0.6895 - val_accuracy: 0.5490 - val_loss: 0.7076\n",
      "Epoch 62/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5907 - loss: 0.6854 - val_accuracy: 0.5452 - val_loss: 0.7092\n",
      "Epoch 63/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5831 - loss: 0.6905 - val_accuracy: 0.5490 - val_loss: 0.7081\n",
      "Epoch 64/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5933 - loss: 0.6863 - val_accuracy: 0.5603 - val_loss: 0.7080\n",
      "Epoch 65/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5831 - loss: 0.6890 - val_accuracy: 0.5578 - val_loss: 0.7073\n",
      "Epoch 66/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5953 - loss: 0.6869 - val_accuracy: 0.5415 - val_loss: 0.7094\n",
      "Epoch 67/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6004 - loss: 0.6814 - val_accuracy: 0.5477 - val_loss: 0.7077\n",
      "Epoch 68/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5838 - loss: 0.6889 - val_accuracy: 0.5490 - val_loss: 0.7077\n",
      "Epoch 69/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5912 - loss: 0.6819 - val_accuracy: 0.5641 - val_loss: 0.7073\n",
      "Epoch 70/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.6019 - loss: 0.6805 - val_accuracy: 0.5515 - val_loss: 0.7077\n",
      "Epoch 70: early stopping\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5749 - loss: 0.6931\n",
      "Batch Size: 32, Learning Rate: 0.0001, Test Accuracy: 0.5653923749923706\n",
      "   Batch Size  Learning Rate  Test Accuracy\n",
      "0           1         0.0001       0.581489\n",
      "1           4         0.0001       0.554326\n",
      "2           8         0.0001       0.589537\n",
      "3          16         0.0001       0.575453\n",
      "4          32         0.0001       0.565392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "learning_rates = [0.0001]\n",
    "\n",
    "results = []\n",
    "for batch_size, learning_rate in product(batch_sizes, learning_rates):\n",
    "    model = Sequential([\n",
    "        GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(64, activation='relu', kernel_initializer=glorot_normal(), kernel_regularizer=regularizers.l2(0.003)),\n",
    "        Dense(32, activation='relu', kernel_initializer=glorot_normal(), kernel_regularizer=regularizers.l2(0.003)),\n",
    "        Dense(16, activation='relu', kernel_initializer=glorot_normal(), kernel_regularizer=regularizers.l2(0.003)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping])\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Batch Size: {batch_size}, Learning Rate: {learning_rate}, Test Accuracy: {accuracy}')\n",
    "    \n",
    "    results.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Learning Rate': learning_rate,\n",
    "        'Test Accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73477ee",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정 후 LSTM vs GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1adeac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: early stopping\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "정확도: 0.5358223915100098\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "정밀도: 0.56797583081571\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential([\n",
    "    GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(32, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(16, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=8, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'정확도: {accuracy}')\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 모델의 예측값 계산\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "print(f'정밀도: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62346cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "정확도: 0.49848636984825134, 정밀도: 0.528169014084507\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(64, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(32, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(16, activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=8, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "_, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# 테스트 데이터로 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f'정확도: {accuracy}, 정밀도: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10d9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9492cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ee62c1c990>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1eeae91c690>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training and Validation Loss')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1eeae95d350>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델 훈련 시 반환된 history 객체에서 loss와 val_loss 추출\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# epoch 수\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# train_loss와 val_loss 시각화\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea13f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08cba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmy_env",
   "language": "python",
   "name": "kmy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "424.831px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
